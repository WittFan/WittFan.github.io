<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Empirical Asset Pricing via Machine Learning]]></title>
    <url>%2F2019%2F09%2F04%2FEmpirical-Asset-Pricing-via-Machine-Learning%2F</url>
    <content type="text"><![CDATA[Shihao GuBryan KellyDacheng Xiu methods：item is asset (one asset level)1 predictors –&gt; return –&gt; portfolio selection 2 predictors –&gt; covariance –&gt; portfolio selection Can Machine Learning-Based Portfolios Outperform Traditional Risk-Based Portfolios? 3 predictors –&gt; return covariance, skewness… –&gt; portfolio selection item is portfolio (portfolios level)4 preditors –&gt; portfolios’ return –&gt; portfolio selection5 predictors –&gt; portfolios’ attributes –&gt; portfolio selection]]></content>
      <categories>
        <category>economics/finance/asset pricing</category>
      </categories>
      <tags>
        <tag>asset pricing</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.covariance]]></title>
    <url>%2F2019%2F08%2F26%2Fsklearn-covariance%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[如何让一本书囊括金融工程]]></title>
    <url>%2F2019%2F08%2F26%2F%E5%A6%82%E4%BD%95%E8%AE%A9%E4%B8%80%E6%9C%AC%E4%B9%A6%E5%9B%8A%E6%8B%AC%E9%87%91%E8%9E%8D%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[实变函数论（实分析）泛函分析凸分析随机过程微分方程统计学动态资产定价博弈论智能优化算法计量经济学机器学习python 套利模型]]></content>
      <categories>
        <category>想法</category>
      </categories>
      <tags>
        <tag>杂思妄想</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[conda的包路径查找]]></title>
    <url>%2F2019%2F08%2F12%2Fconda%E7%9A%84%E5%8C%85%E8%B7%AF%E5%BE%84%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[如果找不到display路径可以通过envs/环境变量/conda-meta/mpld3-0.3-py35_0.json文件找到安装路径找到”paths_data”从而找到{ “path”: “lib/python3.5/site-packages/mpld3/__pycache/_display.cpython-35.pyc”, “path_type”: “hardlink”, “sha256”: “3cd28fc146c93bbb8e211212d4308494e210ccb56ece4976bc60bfef00cc1a27”, “sha256_in_prefix”: “3cd28fc146c93bbb8e211212d4308494e210ccb56ece4976bc60bfef00cc1a27”, “size_in_bytes”: 16226 },]]></content>
      <categories>
        <category>programming/python/conda</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>conda</tag>
        <tag>包路径</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mpld3._display.NumpyEncoder报错]]></title>
    <url>%2F2019%2F08%2F12%2Fmpld3-display-NumpyEncoder%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[在使用mpld3.display.NumpyEncoder过程中报错 12345678910111213141516Traceback (most recent call last):File "ttt.py", line 55, in htmlfil.write(mpld3.fig_to_html(fig))File "/usr/local/lib/python2.7/dist-packages/mpld3/_display.py", line 251, in fig_to_htmlfigure_json=json.dumps(figure_json, cls=NumpyEncoder),File "/usr/lib/python2.7/json/init.py", line 250, in dumpssort_keys=sort_keys, **kw).encode(obj)File "/usr/lib/python2.7/json/encoder.py", line 207, in encodechunks = self.iterencode(o, _one_shot=True)File "/usr/lib/python2.7/json/encoder.py", line 270, in iterencodereturn _iterencode(o, 0)File "/usr/local/lib/python2.7/dist-packages/mpld3/_display.py", line 138, in defaultreturn json.JSONEncoder.default(self, obj)File "/usr/lib/python2.7/json/encoder.py", line 184, in defaultraise TypeError(repr(o) + " is not JSON serializable")TypeError: array([ 1.]) is not JSON serializable 可以在mpld3._display()中修改NumpyEncoder类使用anaconda3用户的包在下面这个位置/Users/用户名/anaconda3/envs/虚拟环境名/lib/python3.5/site-packages/mpld3 12345678910111213class NumpyEncoder(json.JSONEncoder): """ Special json encoder for numpy types """ def default(self, obj): if isinstance(obj, (numpy.int_, numpy.intc, numpy.intp, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16,numpy.uint32, numpy.uint64)): return int(obj) elif isinstance(obj, (numpy.float_, numpy.float16, numpy.float32, numpy.float64)): return float(obj) elif isinstance(obj,(numpy.ndarray,)): #### This is the fix return obj.tolist() return json.JSONEncoder.default(self, obj) 如果找不到_display路径可以通过envs/环境变量/conda-meta/mpld3-0.3-py35_0.json文件找到安装路径找到”paths_data”从而找到{&quot;_path&quot;: &quot;lib/python3.5/site-packages/mpld3/__pycache__/_display.cpython-35.pyc&quot;, &quot;path_type&quot;: &quot;hardlink&quot;, &quot;sha256&quot;: &quot;3cd28fc146c93bbb8e211212d4308494e210ccb56ece4976bc60bfef00cc1a27&quot;, &quot;sha256_in_prefix&quot;: &quot;3cd28fc146c93bbb8e211212d4308494e210ccb56ece4976bc60bfef00cc1a27&quot;, &quot;size_in_bytes&quot;: 16226 },]]></content>
      <categories>
        <category>programming/python/mpld3</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>mpld3</tag>
        <tag>_display</tag>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协方差(相关性)、协整、回归系数]]></title>
    <url>%2F2019%2F08%2F12%2F%E5%8D%8F%E6%96%B9%E5%B7%AE-%E7%9B%B8%E5%85%B3%E6%80%A7-%E3%80%81%E5%8D%8F%E6%95%B4%E3%80%81%E5%9B%9E%E5%BD%92%E7%B3%BB%E6%95%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[线性回归、逻辑回归、岭回归、Lesso回归、非均衡数据集的损失函数]]></title>
    <url>%2F2019%2F08%2F12%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E3%80%81%E4%BA%8C%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E3%80%81%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E3%80%81%E5%B2%AD%E5%9B%9E%E5%BD%92%E3%80%81Lesso%E5%9B%9E%E5%BD%92%E3%80%81%E9%9D%9E%E5%9D%87%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"></content>
      <categories>
        <category>math/statistics</category>
      </categories>
      <tags>
        <tag>linear regression</tag>
        <tag>logit regression</tag>
        <tag>multinomial logit regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas read_table]]></title>
    <url>%2F2019%2F08%2F09%2Fpandas-read-table%2F</url>
    <content type="text"><![CDATA[pandas.read_csv参数详解pandas.read_csv参数整理 读取CSV（逗号分割）文件到DataFrame也支持文件的部分导入和选择迭代更多帮助参见：http://pandas.pydata.org/pandas-docs/stable/io.html参数： filepath_or_buffer : str，pathlib。str, pathlib.Path, py._path.local.LocalPath or any object with a read() method (such as a file handle or StringIO)可以是URL，可用URL类型包括：http, ftp, s3和文件。对于多文件正在准备中本地文件读取实例：://localhost/path/to/table.csv sep : str, default ‘,’指定分隔符。如果不指定参数，则会尝试使用逗号分隔。分隔符长于一个字符并且不是‘\s+’,将使用python的语法分析器。并且忽略数据中的逗号。正则表达式例子：’\r\t’ delimiter : str, default None定界符，备选分隔符（如果指定该参数，则sep参数失效） delim_whitespace : boolean, default False.指定空格(例如’ ‘或者’ ‘)是否作为分隔符使用，等效于设定sep=’\s+’。如果这个参数设定为Ture那么delimiter 参数失效。在新版本0.18.1支持 header : int or list of ints, default ‘infer’指定行数用来作为列名，数据开始行数。如果文件中没有列名，则默认为0，否则设置为None。如果明确设定header=0 就会替换掉原来存在列名。header参数可以是一个list例如：[0,1,3]，这个list表示将文件中的这些行作为列标题（意味着每一列有多个标题），介于中间的行将被忽略掉（例如本例中的2；本例中的数据1,2,4行将被作为多级标题出现，第3行数据将被丢弃，dataframe的数据从第5行开始。）。注意：如果skip_blank_lines=True 那么header参数忽略注释行和空行，所以header=0表示第一行数据而不是文件的第一行。 names : array-like, default None用于结果的列名列表，如果数据文件中没有列标题行，就需要执行header=None。默认列表中不能出现重复，除非设定参数mangle_dupe_cols=True。 index_col : int or sequence or False, default None用作行索引的列编号或者列名，如果给定一个序列则有多个行索引。如果文件不规则，行尾有分隔符，则可以设定index_col=False 来是的pandas不适用第一列作为行索引。 usecols : array-like, default None返回一个数据子集，该列表中的值必须可以对应到文件中的位置（数字可以对应到指定的列）或者是字符传为文件中的列名。例如：usecols有效参数可能是 [0,1,2]或者是 [‘foo’, ‘bar’, ‘baz’]。使用这个参数可以加快加载速度并降低内存消耗。 as_recarray : boolean, default False不赞成使用：该参数会在未来版本移除。请使用pd.read_csv(…).to_records()替代。返回一个Numpy的recarray来替代DataFrame。如果该参数设定为True。将会优先squeeze参数使用。并且行索引将不再可用，索引列也将被忽略。 squeeze : boolean, default False如果文件值包含一列，则返回一个Series prefix : str, default None在没有列标题时，给列添加前缀。例如：添加‘X’ 成为 X0, X1, … mangle_dupe_cols : boolean, default True重复的列，将‘X’…’X’表示为‘X.0’…’X.N’。如果设定为false则会将所有重名列覆盖。 dtype : Type name or dict of column -&gt; type, default None每列数据的数据类型。例如 {‘a’: np.float64, ‘b’: np.int32} engine : {‘c’, ‘python’}, optionalParser engine to use. The C engine is faster while the python engine is currently more feature-complete.使用的分析引擎。可以选择C或者是python。C引擎快但是Python引擎功能更加完备。 converters : dict, default None列转换函数的字典。key可以是列名或者列的序号。 true_values : list, default NoneValues to consider as True false_values : list, default NoneValues to consider as False skipinitialspace : boolean, default False忽略分隔符后的空白（默认为False，即不忽略）. skiprows : list-like or integer, default None需要忽略的行数（从文件开始处算起），或需要跳过的行号列表（从0开始）。 skipfooter : int, default 0从文件尾部开始忽略。 (c引擎不支持) skip_footer : int, default 0不推荐使用：建议使用skipfooter ，功能一样。 nrows : int, default None需要读取的行数（从文件头开始算起）。 na_values : scalar, str, list-like, or dict, default None一组用于替换NA/NaN的值。如果传参，需要制定特定列的空值。默认为‘1.#IND’, ‘1.#QNAN’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘nan’`. keep_default_na : bool, default True如果指定na_values参数，并且keep_default_na=False，那么默认的NaN将被覆盖，否则添加。 na_filter : boolean, default True是否检查丢失值（空字符串或者是空值）。对于大文件来说数据集中没有空值，设定na_filter=False可以提升读取速度。 verbose : boolean, default False是否打印各种解析器的输出信息，例如：“非数值列中缺失值的数量”等。 skip_blank_lines : boolean, default True如果为True，则跳过空行；否则记为NaN。 parse_dates : boolean or list of ints or names or list of lists or dict, default Falseboolean. True -&gt; 解析索引list of ints or names. e.g. If [1, 2, 3] -&gt; 解析1,2,3列的值作为独立的日期列；list of lists. e.g. If [[1, 3]] -&gt; 合并1,3列作为一个日期列使用dict, e.g. {‘foo’ : [1, 3]} -&gt; 将1,3列合并，并给合并后的列起名为”foo” infer_datetime_format : boolean, default False如果设定为True并且parse_dates 可用，那么pandas将尝试转换为日期类型，如果可以转换，转换方法并解析。在某些情况下会快5~10倍。 keep_date_col : boolean, default False如果连接多列解析日期，则保持参与连接的列。默认为False。 date_parser : function, default None用于解析日期的函数，默认使用dateutil.parser.parser来做转换。Pandas尝试使用三种不同的方式解析，如果遇到问题则使用下一种方式。1.使用一个或者多个arrays（由parse_dates指定）作为参数；2.连接指定多列字符串作为一个列作为参数；3.每行调用一次date_parser函数来解析一个或者多个字符串（由parse_dates指定）作为参数。 dayfirst : boolean, default FalseDD/MM格式的日期类型 iterator : boolean, default False返回一个TextFileReader 对象，以便逐块处理文件。 chunksize : int, default None文件块的大小， See IO Tools docs for more informationon iterator and chunksize. compression : {‘infer’, ‘gzip’, ‘bz2’, ‘zip’, ‘xz’, None}, default ‘infer’直接使用磁盘上的压缩文件。如果使用infer参数，则使用 gzip, bz2, zip或者解压文件名中以‘.gz’, ‘.bz2’, ‘.zip’, or ‘xz’这些为后缀的文件，否则不解压。如果使用zip，那么ZIP包中国必须只包含一个文件。设置为None则不解压。新版本0.18.1版本支持zip和xz解压 thousands : str, default None千分位分割符，如“，”或者“.” decimal : str, default ‘.’字符中的小数点 (例如：欧洲数据使用’，‘). float_precision : string, default NoneSpecifies which converter the C engine should use for floating-point values. The options are None for the ordinary converter, high for the high-precision converter, and round_trip for the round-trip converter.指定 lineterminator : str (length 1), default None行分割符，只在C解析器下使用。 quotechar : str (length 1), optional引号，用作标识开始和解释的字符，引号内的分割符将被忽略。 quoting : int or csv.QUOTE_* instance, default 0控制csv中的引号常量。可选 QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3) doublequote : boolean, default True双引号，当单引号已经被定义，并且quoting 参数不是QUOTE_NONE的时候，使用双引号表示引号内的元素作为一个元素使用。 escapechar : str (length 1), default None当quoting 为QUOTE_NONE时，指定一个字符使的不受分隔符限值。 comment : str, default None标识着多余的行不被解析。如果该字符出现在行首，这一行将被全部忽略。这个参数只能是一个字符，空行（就像skip_blank_lines=True）注释行被header和skiprows忽略一样。例如如果指定comment=’#’ 解析‘#empty\na,b,c\n1,2,3’ 以header=0 那么返回结果将是以’a,b,c’作为header。 encoding : str, default None指定字符集类型，通常指定为’utf-8’. List of Python standard encodings dialect : str or csv.Dialect instance, default None如果没有指定特定的语言，如果sep大于一个字符则忽略。具体查看csv.Dialect 文档 tupleize_cols : boolean, default FalseLeave a list of tuples on columns as is (default is to convert to a Multi Index on the columns) error_bad_lines : boolean, default True如果一行包含太多的列，那么默认不会返回DataFrame ，如果设置成false，那么会将改行剔除（只能在C解析器下使用）。 warn_bad_lines : boolean, default True如果error_bad_lines =False，并且warn_bad_lines =True 那么所有的“bad lines”将会被输出（只能在C解析器下使用）。 low_memory : boolean, default True分块加载到内存，再低内存消耗中解析。但是可能出现类型混淆。确保类型不被混淆需要设置为False。或者使用dtype 参数指定类型。注意使用chunksize 或者iterator 参数分块读入会将整个文件读入到一个Dataframe，而忽略类型（只能在C解析器中有效） buffer_lines : int, default None不推荐使用，这个参数将会在未来版本移除，因为他的值在解析器中不推荐使用 compact_ints : boolean, default False不推荐使用，这个参数将会在未来版本移除如果设置compact_ints=True ，那么任何有整数类型构成的列将被按照最小的整数类型存储，是否有符号将取决于use_unsigned 参数 use_unsigned : boolean, default False不推荐使用：这个参数将会在未来版本移除如果整数列被压缩(i.e. compact_ints=True)，指定被压缩的列是有符号还是无符号的。memory_map : boolean, default False如果使用的文件在内存内，那么直接map文件使用。使用这种方式可以避免文件再次进行IO操作。https://blog.csdn.net/u012131430/article/details/78299582]]></content>
  </entry>
  <entry>
    <title><![CDATA[商品期货协整配对]]></title>
    <url>%2F2019%2F08%2F09%2F%E5%95%86%E5%93%81%E6%9C%9F%E8%B4%A7%E5%8D%8F%E6%95%B4%E9%85%8D%E5%AF%B9%2F</url>
    <content type="text"><![CDATA[https://uqer.io/v3/community/share/57b52ac0228e5b79aa759416]]></content>
  </entry>
  <entry>
    <title><![CDATA[平稳性检验]]></title>
    <url>%2F2019%2F08%2F09%2F%E5%B9%B3%E7%A8%B3%E6%80%A7%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[python模块: deque]]></title>
    <url>%2F2019%2F08%2F09%2Fpython%E6%A8%A1%E5%9D%97-deque%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F08%2F04%2Ftest2%2F</url>
    <content type="text"><![CDATA[Title document.addEventListener("copy", copy); function copy(oEvent){ var selection = window.getSelection(); var quoteMagic = "\n"; quoteMagic += "\n"; quoteMagic += "\n"; var copytext = quoteMagic + selection + "\n"; oEvent.preventDefault(); oEvent.clipboardData.setData("text", copytext); } 测试1 测试2]]></content>
  </entry>
  <entry>
    <title><![CDATA[为markdown创建一个新标记符]]></title>
    <url>%2F2019%2F08%2F04%2F%E4%B8%BAmarkdown%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BC%95%E6%96%87%E6%A0%87%E8%AE%B0%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[问题的由来当我们在编写markdown文件的时候会经常从网页上ctr+c其他人的文字，而我们标注作者和文章的来源不是特别方便，这会导致许多时候我们不去标注作者和文章的来源。引文规范对于问题追溯和文明发展相当重要，与其道德说教来让大家遵守引文规范，不如提供更方便的方式来让大家遵守，因此，本文抛砖引玉出一种新的高效的规范和工具。 复制时天生携带引文信息将下面的javascript语句放到网页的head处 1234567891011121314&lt;script type="text/javascript"&gt; document.addEventListener("copy", copy); function copy(oEvent)&#123; var selection = window.getSelection(); var quoteMagic = "&lt;!-- It's convenient in markdown file. --&gt;\n"; quoteMagic += "&lt;!-- More information: https://www.exobrain.online/2019/08/04/%E4%B8%BAmarkdown%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BC%95%E6%96%87%E6%A0%87%E8%AE%B0%E7%AC%A6/#more --&gt;\n"; quoteMagic += "&lt;!-- quote(start)&#123; author: authorname, site: www.sitename.com, location: "; quoteMagic += document.location.href; quoteMagic += "&#125; --&gt;\n"; var copytext = quoteMagic + selection + "\n&lt;!-- quote(end) --&gt;"; oEvent.preventDefault(); oEvent.clipboardData.setData("text", copytext); &#125;&lt;/script&gt; 示例如下 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;script type="text/javascript"&gt; document.addEventListener("copy", copy); function copy(oEvent)&#123; var selection = window.getSelection(); var quoteMagic = "&lt;!-- It's convenient in markdown file. --&gt;\n"; quoteMagic += "&lt;!-- More information: https://www.exobrain.online/2019/08/04/%E4%B8%BAmarkdown%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BC%95%E6%96%87%E6%A0%87%E8%AE%B0%E7%AC%A6/#more --&gt;\n"; quoteMagic += "&lt;!-- quote(start)&#123; author: authorname, site: www.sitename.com, location: "; quoteMagic += document.location.href; quoteMagic += "&#125; --&gt;\n"; var copytext = quoteMagic + selection + "\n&lt;!-- quote(end) --&gt;"; oEvent.preventDefault(); oEvent.clipboardData.setData("text", copytext); &#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;测试1&lt;br&gt;测试2&lt;/body&gt;&lt;/html&gt; 当我们运行网页，复制网页中内容后，将其粘贴到mardown文件，会得到下面的结果 而其（在vscode+MarkDownPreview中）渲染成html的显示效果如下图这样我们就可以在不影响显示的情况下保留其引文信息。当然，html中保留引文信息的工作需要在markdown文件转换为html文件的过程中完成，转换工具和规范这里不做讨论，先留下个坑，或者其他人来完成吧，😃。 hexo的next主题下复制时附加引文信息找到hexo项目根目录，我的根目录是/blog在 ../blog/themes/next/layout/_partials/head目录下找到custom-head.swig文件，将下面的代码添加到文末就可以了。 1234567891011121314&lt;script type="text/javascript"&gt; document.addEventListener("copy", copy); function copy(oEvent)&#123; var selection = window.getSelection(); var quoteMagic = "&lt;!-- It's convenient in markdown file. --&gt;\n"; quoteMagic += "&lt;!-- More information: https://www.exobrain.online/2019/08/04/%E4%B8%BAmarkdown%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BC%95%E6%96%87%E6%A0%87%E8%AE%B0%E7%AC%A6/#more --&gt;\n"; quoteMagic += "&lt;!-- quote(start)&#123; author: authorname, site: www.sitename.com, location: "; quoteMagic += document.location.href; quoteMagic += "&#125; --&gt;\n"; var copytext = quoteMagic + selection + "\n&lt;!-- quote(end) --&gt;"; oEvent.preventDefault(); oEvent.clipboardData.setData("text", copytext); &#125;&lt;/script&gt; 将hexo-next下的文末出处去掉将文件../blog/themes/next/_config.yml（blog为我的hexo根目录）中的post_copyright项下的enable的值从true改为false 12345# Declare license on postspost_copyright: enable: true license: CC BY-NC-SA 3.0 license_url: https://creativecommons.org/licenses/by-nc-sa/3.0/ 改完后为 12345# Declare license on postspost_copyright: enable: false license: CC BY-NC-SA 3.0 license_url: https://creativecommons.org/licenses/by-nc-sa/3.0/ 之后文章末尾就不会再出现下图的情形了 如果使用了代码复制功能怎么改当使用代码复制功能的时候，我们希望复制到干净的代码，却不想得到引用信息;而在复制文章内容的时候，我们希望得到引文信息。 笔者使用的是这个链接下实现的代码复制功能，https://yfzhou.coding.me/2018/08/27/Hexo-Next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E4%BB%A3%E7%A0%81%E5%9D%97%E5%A4%8D%E5%88%B6%E5%8A%9F%E8%83%BD%EF%BC%89/ 我的办法是：将clipboard-use.js文件的代码进行了一下升级，加了if语句。可以理解成将custom-head.swig文件里的那个添加引用的代码整合到了clipboard-use.js文件里。 1234567891011121314151617181920212223242526272829303132333435363738394041 /*页面载入完成后，创建复制按钮*/var btnCopyEmpty = true;document.addEventListener("copy", copy);function copy(oEvent)&#123; var selection = window.getSelection(); if(btnCopyEmpty)&#123; console.log(btnCopyEmpty) var quoteMagic = "&lt;!-- It's convenient in markdown file. --&gt;\n"; quoteMagic += "&lt;!-- More information: https://www.exobrain.online/2019/08/04/%E4%B8%BAmarkdown%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BC%95%E6%96%87%E6%A0%87%E8%AE%B0%E7%AC%A6/#more --&gt;\n"; quoteMagic += "&lt;!-- quote(start)&#123; author: authorname, site: www.sitename.com, location: "; quoteMagic += document.location.href; quoteMagic += "&#125; --&gt;\n"; var copytext = quoteMagic + selection + "\n&lt;!-- quote(end) --&gt;"; &#125;else&#123; var copytext = selection; &#125; oEvent.preventDefault(); oEvent.clipboardData.setData("text", copytext);&#125;;!function (e, t, a) &#123; /* code */var initCopyCode = function()&#123; var copyHtml = ''; copyHtml += '&lt;button class="btn-copy" data-clipboard-snippet=""&gt;'; //fa fa-globe可以去字体库替换自己想要的图标copyHtml += ' &lt;i class="fa fa-clipboard"&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;'; copyHtml += '&lt;/button&gt;'; $(".highlight .code pre").before(copyHtml); new ClipboardJS('.btn-copy', &#123; target: function(trigger) &#123; btnCopyEmpty = false; return trigger.nextElementSibling; &#125; &#125;); // clipboard.on('success', function(e)&#123; // btnCopyEmpty = false; // &#125;);&#125;initCopyCode();&#125;(window, document);]]></content>
      <categories>
        <category>programming/nodejs/hexo</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>javascript</tag>
        <tag>quote</tag>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test password]]></title>
    <url>%2F2019%2F08%2F04%2Ftest-password%2F</url>
    <content type="text"></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django & React: JWT Authentication]]></title>
    <url>%2F2019%2F06%2F10%2FDjango-React-JWT-Authentication%2F</url>
    <content type="text"><![CDATA[https://medium.com/@dakota.lillie/django-react-jwt-authentication-5015ee00ef9a Recently I’ve been building an app using a combination of React for the frontend and Django for the backend. I wasn’t previously all too familiar with Django, but this seemed like a good opportunity to teach myself, and having experience with Ruby on Rails has made the process a little easier. However, Django and Rails have their fair share of differences, and one of the things which I’ve had the most trouble implementing is user authentication. So I’m sharing what I’ve learned, in the hopes that you too might find it useful!We’ll be building a demo app, for which you can find the finalized code for the frontend here and for the backend here. Also, I should mention that I’m currently working with Django 2.0.4, and React 16.3.The Basic Premise: Sessions vs. TokensThere are many different potential approaches to implementing authentication. Here I’ll just cover two of the most common ones: session authentication (via cookies), and token authentication. There are plenty of articles around the internet differentiating the two, but I’ll give a quick summary of what they are and how they work.Session authentication is stateful, which means when a user logs in, data pertaining to their authenticated status gets stored either in memory or a database. This data is collectively referred to as a session, and to facilitate its access, a cookie with the user’s session ID is sent back to the client and stored in the browser. Then, next time the client makes a request, that cookie is included in the request and the server searches for a session that matches the cookie’s session ID. If there’s a match, then the backend proceeds to process the request. When the user logs out, another request has to be made to the server so that the relevant session data is destroyed, along with the cookie in the browser’s storage.Session authentication was for a long time considered the preferred approach, and it remains widely used. However, it suffers from several notable drawbacks. Most relevant to our particular circumstance is the fact that cookies are tied to a particular domain, which leads to significant CORS headaches when the front and back ends are decoupled.That leads us to token authentication. Tokens are key-value pairs which usually live in the local storage of your browser. In this regard they are similar to cookies — however, where session authentication was stateful, token authorization is stateless, meaning there’s no record kept on the server of which users are logged in, how many tokens have been issued etc. Instead, tokens are generated by means of a complex encryption process which, when reversed and decrypted, authenticates the user.This is a very broad generalization of the methodology employed by JSON Web Tokens (JWTs for short). JWTs are regarded as the gold standard in authentication right now, so that’s what we’ll be using today. With that said, let’s write some code!Setting Up DjangoFirst off, we need to set up our virtual environment (if that’s unfamiliar to you, I wrote a whole blog post about it!). I’m going to use pipenv here—make sure you have it installed, then navigate to the directory you want your project to be in and run:pipenv installOnce that’s done, activate the virtual environment with:pipenv shellNow we’re going to need to install some packages, including Django, Django REST framework (hereafter referred to as the DRF), Django REST framework JWT, and Django CORS headers. The DRF is what we’ll be layering on top of Django to turn our project into an API, while Django REST framework JWT gives us the ability to use JWT tokens for our app, and Django CORS headers is necessary to avoid CORS issues:pipenv install djangopipenv install djangorestframeworkpipenv install djangorestframework-jwtpipenv install django-cors-headersOnce this is done, we’re ready to create the Django project. Run the following:django-admin startproject mysite .Note the period at the end there — that denotes that we want to create the project with the current directory as as the root, rather than putting it in a new subdirectory. At this point, your project structure should look something like this:Now we need to adjust some of our project’s settings. Go into settings.py and make the following modifications: 12345678910111213141516171819202122232425262728293031# ...INSTALLED_APPS = [ # ... 'rest_framework', 'corsheaders',]MIDDLEWARE = [ # ... 'corsheaders.middleware.CorsMiddleware', # Note that this needs to be placed above CommonMiddleware 'django.middleware.common.CommonMiddleware', # This should already exist # ...]#...REST_FRAMEWORK = &#123; 'DEFAULT_PERMISSION_CLASSES': ( 'rest_framework.permissions.IsAuthenticated', ), 'DEFAULT_AUTHENTICATION_CLASSES': ( 'rest_framework_jwt.authentication.JSONWebTokenAuthentication', 'rest_framework.authentication.SessionAuthentication', 'rest_framework.authentication.BasicAuthentication', ),&#125;CORS_ORIGIN_WHITELIST = ( 'localhost:3000',) Let’s examine what we’ve done here: first, we’ve registered the DRF and CORS headers packages with our project by adding them to INSTALLED_APPS. Then, we added a piece of custom CORS middleware, making sure to place it place it above any middleware that generates responses such as Django’s CommonMiddleware. We then customized some of the default settings for the DRF: first, we set the DEFAULT_PERMISSION_CLASSES, which in this case will require a request to be authenticated before it is processed unless specified otherwise. Then, we set the DEFAULT_AUTHENTICATION_CLASSES, which determines which authentication methods the server will try when it receives a request, in descending order. Finally, we added localhost:3000 to the CORS_ORIGIN_WHITELIST, since that’s where the requests from our React app will be coming from.The DRF JWT package provides us with a default view for decoding received JWTs. We can add that to urls.py: 12345678#...from rest_framework_jwt.views import obtain_jwt_tokenurlpatterns = [ #... path('token-auth/', obtain_jwt_token)] And we’re already almost ready to test whether or not this works! But before we can, we need to create a user, and before do that, we need to apply our migrations. Run the following: python manage.py migrate Once that’s done, the easiest way to create a new user is with: python manage.py createsuperuser Fill in all the fields and you should be good to go. Now if you start your development server: python manage.py runserver and navigate to http://localhost:8000/token-auth/, you should see an html form there with username and password fields (this is a convenience provided by the DRF… isn’t it awesome?). Fill in these fields and you should see the JWT itself displayed right there on the page. This takes care of logging in but we still need our users to be able to sign up. Fortunately, Django comes with a built in User model that we can use (which is easy enough to customize, should you need to do so). All we need to do is create the view for it. But if we’re making a view, we’re going to need an app to put it in. So let’s do that now:python manage.py startapp coreI’m calling the app “core”, but you can of course call it whatever you want. Before we do anything else, let’s register the app in settings.py: 12345678# ...INSTALLED_APPS = [ # ... 'core.apps.CoreConfig']# ... Now we need to take a few steps that might seem circuitous at first but which I promise will make sense by the end. First, we need to create a couple serializers for our User model. These serializers will be responsible for serializing/unserializing the User model into and out of various formats, primarily JSON in our case. Go ahead and create a new core/serializers.py file, and fill it with the following: 123456789101112131415161718192021222324252627282930313233343536from rest_framework import serializersfrom rest_framework_jwt.settings import api_settingsfrom django.contrib.auth.models import Userclass UserSerializer(serializers.ModelSerializer): class Meta: model = User fields = ('username',)class UserSerializerWithToken(serializers.ModelSerializer): token = serializers.SerializerMethodField() password = serializers.CharField(write_only=True) def get_token(self, obj): jwt_payload_handler = api_settings.JWT_PAYLOAD_HANDLER jwt_encode_handler = api_settings.JWT_ENCODE_HANDLER payload = jwt_payload_handler(obj) token = jwt_encode_handler(payload) return token def create(self, validated_data): password = validated_data.pop('password', None) instance = self.Meta.model(**validated_data) if password is not None: instance.set_password(password) instance.save() return instance class Meta: model = User fields = ('token', 'username', 'password') The reason we’re making two different serializers for the model is because we’ll be using the UserSerializerWithToken for handling signups. When a user signs up, we want the response from the server to include both their relevant user data (in this case, just the username), as well as the token, which will be stored in the browser for further authentication. But we don’t need the token every time we request a user’s data — just when signing up. Thus, separate serializers.That’s the ‘why’, but let’s take a closer look at the code for the ‘how’. Both serializers inherit from rest_framework.serializers.ModelSerializer, which provides us with a handy shortcut for customizing the serializers according to the model data they’ll be working with (otherwise we’d need to spell out every field by hand). In the internal Meta class, we indicate which model each serializer will be representing, and which fields from that model we want the serializer to include.But the User class doesn’t have an internal ‘token’ field, so for that we do need to define our own custom field. We define the token variable to be a custom method, then add a get_token() method which handles the manual creation of a new token. It does this using the default settings for payload and encoding handling provided by the JWT package (the payload is the data being tokenized, in this case the user). Finally, we added the custom ‘token’ field to the fields variable in our Meta internal class.We also need to make sure the serializer recognizes and stores the submitted password, but doesn’t include it in the returned JSON. So we add the ‘password’ field to fields, but above that also specify that the password should be write only. Then, we override the serializer’s create() method, which determines how the object being serialized gets saved to the database. We do this primarily so that we can call the set_password() method on the user instance, which is how the password gets properly hashed.Now we’re ready to start configuring our views in core/views.py. There are many ways of going about doing this (function-based views, class-based views, or viewsets). Since viewsets can be confusing if you don’t understand what’s happening internally (and since we don’t have enough views here to reap their benefits anyway), I’ll use the other forms of views here: 123456789101112131415161718192021222324252627282930313233from django.http import HttpResponseRedirectfrom django.contrib.auth.models import Userfrom rest_framework import permissions, statusfrom rest_framework.decorators import api_viewfrom rest_framework.response import Responsefrom rest_framework.views import APIViewfrom .serializers import UserSerializer, UserSerializerWithToken@api_view(['GET'])def current_user(request): """ Determine the current user by their token, and return their data """ serializer = UserSerializer(request.user) return Response(serializer.data)class UserList(APIView): """ Create a new user. It's called 'UserList' because normally we'd have a get method here too, for retrieving a list of all User objects. """ permission_classes = (permissions.AllowAny,) def post(self, request, format=None): serializer = UserSerializerWithToken(data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data, status=status.HTTP_201_CREATED) return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) Let’s walk through this. First, we have the current_user function-based view. This view will be used anytime the user revisits the site, reloads the page, or does anything else that causes React to forget its state. React will check if the user has a token stored in the browser, and if a token is found, it’ll make a request to this view. Since we’ve set things up properly, the token will be parsed automatically to check for authentication, and if validated we’ll receive the user object associated with that token in the request’s user property. We can then serialize the user object, and return the data from the serializer in the response. This whole function then serves as the input for the @api_view decorator, which specifies the request methods this view will respond to (in this case, just GET requests).Next, we have our class-based UserList view. When a request is routed to this view, a UserSerializerWithToken serializer object is instantiated with the data the user entered into the signup form. The serializer checks whether or not the data is valid, and if it is, it’ll save the new user and return that user’s data in the response (including the token, since we’re using this particular serializer). Note that we specify the permissions for this class to be permissions.AllowAny, because otherwise, the user would have to be logged in before they could sign up, which could be frustrating.We have our views now, but as of yet still no way of accessing them. To do that, we need to assign them some routes. It’s customary to give each app its own route configuration, so create a new core/urls.py file and edit it like so: 1234567from django.urls import pathfrom .views import current_user, UserListurlpatterns = [ path('current_user/', current_user), path('users/', UserList.as_view())] Now we’ll hook up this file to the root urls conf by editing that file (mysite/urls.py): 1234567#...from django.urls import path, includeurlpatterns = [ #... path('core/', include('core.urls'))] With that, we’re almost good to go — but we still have a problem. Currently, when a user logs in, they receive their token but not any of their user data. To remedy this, we could make a separate request to the current_user() view we defined earlier… but that’s annoying. Why make multiple requests? Instead, let’s customize our JWT settings a bit.What we’re going to need to do is define a custom JWT response payload handler which includes the user’s serialized data. Within the mysite directory, make a new file called utils.py and fill it with the following: 12345678from core.serializers import UserSerializerdef my_jwt_response_handler(token, user=None, request=None): return &#123; 'token': token, 'user': UserSerializer(user, context=&#123;'request': request&#125;).data &#125; All this is doing is adding a new ‘user’ field with the user’s serialized data when a token is generated. This is going to be our new default JWT response handler, which we can set up by adding a little bit to our settings.py file: 12345# ...JWT_AUTH = &#123; 'JWT_RESPONSE_PAYLOAD_HANDLER': 'mysite.utils.my_jwt_response_handler'&#125; Now, when a user logs in, they’ll get all their user data along with their token… And we should finally be done! On to the frontend. One additional point: JWT encoding/decoding involves the use of a secret key, which defaults to the SECRET_KEY constant defined in settings.py. If you’re deploying an app that uses JWT to production, be sure to change this, or at least hide the secret key from Github.]]></content>
      <categories>
        <category>programming/python/django</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>djangoRestfulFramework</tag>
        <tag>JWT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stock Market Indicators]]></title>
    <url>%2F2018%2F10%2F22%2FStock-Market-Indicators%2F</url>
    <content type="text"><![CDATA[https://github.com/voice32/stock_market_indicatorsStock Market IndicatorsA small Python library with most the common stock market indicators. RequirementsPandasNumpyInstallationClone or download the indicators.py file into your project directory. UsageImport the module: import indicators The functions in this library accept the data in Pandas DataFrame format. The data should contain OPEN, HIGH, LOW, CLOSE and VOLUME columns. See the comments for each function for the list of required columns. Their default names are hardcoded in functions’ params, however you may supply your own column names, if they are different. Sometimes you would also need to provide periods over which to calculate the indicator values. However, for all of them the default (recommended) values are pre-assigned. List of implemented techinical indicatorsExponential moving average (EMA)Moving Average Convergence/Divergence Oscillator (MACD)Accumulation Distribution (A/D)On Balance Volume (OBV)Price-volume trend (PVT)Average true range (ATR)Bollinger BandsChaikin OscillatorTypical PriceEase of MovementMass IndexAverage directional movement indexMoney Flow Index (MFI)Negative Volume Index (NVI)Positive Volume Index (PVI)MomentumRelative Strenght Index (RSI)Chaikin Volatility (CV)William’s Accumulation/DistributionWilliam’s % RTRIXUltimate Oscillator 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690"""Copyright, Rinat Maksutov, 2017.License: GNU General Public License"""import numpy as npimport pandas as pd"""Exponential moving averageSource: http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:moving_averagesParams: data: pandas DataFrame period: smoothing period column: the name of the column with values for calculating EMA in the 'data' DataFrame Returns: copy of 'data' DataFrame with 'ema[period]' column added"""def ema(data, period=0, column='&lt;CLOSE&gt;'): data['ema' + str(period)] = data[column].ewm(ignore_na=False, min_periods=period, com=period, adjust=True).mean() return data"""Moving Average Convergence/Divergence Oscillator (MACD)Source: http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:moving_average_convergence_divergence_macdParams: data: pandas DataFrame period_long: the longer period EMA (26 days recommended) period_short: the shorter period EMA (12 days recommended) period_signal: signal line EMA (9 days recommended) column: the name of the column with values for calculating MACD in the 'data' DataFrame Returns: copy of 'data' DataFrame with 'macd_val' and 'macd_signal_line' columns added"""def macd(data, period_long=26, period_short=12, period_signal=9, column='&lt;CLOSE&gt;'): remove_cols = [] if not 'ema' + str(period_long) in data.columns: data = ema(data, period_long) remove_cols.append('ema' + str(period_long)) if not 'ema' + str(period_short) in data.columns: data = ema(data, period_short) remove_cols.append('ema' + str(period_short)) data['macd_val'] = data['ema' + str(period_short)] - data['ema' + str(period_long)] data['macd_signal_line'] = data['macd_val'].ewm(ignore_na=False, min_periods=0, com=period_signal, adjust=True).mean() data = data.drop(remove_cols, axis=1) return data"""Accumulation Distribution Source: http://stockcharts.com/school/doku.php?st=accumulation+distribution&amp;id=chart_school:technical_indicators:accumulation_distribution_lineParams: data: pandas DataFrame trend_periods: the over which to calculate AD open_col: the name of the OPEN values column high_col: the name of the HIGH values column low_col: the name of the LOW values column close_col: the name of the CLOSE values column vol_col: the name of the VOL values column Returns: copy of 'data' DataFrame with 'acc_dist' and 'acc_dist_ema[trend_periods]' columns added"""def acc_dist(data, trend_periods=21, open_col='&lt;OPEN&gt;', high_col='&lt;HIGH&gt;', low_col='&lt;LOW&gt;', close_col='&lt;CLOSE&gt;', vol_col='&lt;VOL&gt;'): for index, row in data.iterrows(): if row[high_col] != row[low_col]: ac = ((row[close_col] - row[low_col]) - (row[high_col] - row[close_col])) / (row[high_col] - row[low_col]) * row[vol_col] else: ac = 0 data.set_value(index, 'acc_dist', ac) data['acc_dist_ema' + str(trend_periods)] = data['acc_dist'].ewm(ignore_na=False, min_periods=0, com=trend_periods, adjust=True).mean() return data"""On Balance Volume (OBV)Source: http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:on_balance_volume_obvParams: data: pandas DataFrame trend_periods: the over which to calculate OBV close_col: the name of the CLOSE values column vol_col: the name of the VOL values column Returns: copy of 'data' DataFrame with 'obv' and 'obv_ema[trend_periods]' columns added"""def on_balance_volume(data, trend_periods=21, close_col='&lt;CLOSE&gt;', vol_col='&lt;VOL&gt;'): for index, row in data.iterrows(): if index &gt; 0: last_obv = data.at[index - 1, 'obv'] if row[close_col] &gt; data.at[index - 1, close_col]: current_obv = last_obv + row[vol_col] elif row[close_col] &lt; data.at[index - 1, close_col]: current_obv = last_obv - row[vol_col] else: current_obv = last_obv else: last_obv = 0 current_obv = row[vol_col] data.set_value(index, 'obv', current_obv) data['obv_ema' + str(trend_periods)] = data['obv'].ewm(ignore_na=False, min_periods=0, com=trend_periods, adjust=True).mean() return data"""Price-volume trend (PVT) (sometimes volume-price trend)Source: https://en.wikipedia.org/wiki/Volume%E2%80%93price_trendParams: data: pandas DataFrame trend_periods: the over which to calculate PVT close_col: the name of the CLOSE values column vol_col: the name of the VOL values column Returns: copy of 'data' DataFrame with 'pvt' and 'pvt_ema[trend_periods]' columns added"""def price_volume_trend(data, trend_periods=21, close_col='&lt;CLOSE&gt;', vol_col='&lt;VOL&gt;'): for index, row in data.iterrows(): if index &gt; 0: last_val = data.at[index - 1, 'pvt'] last_close = data.at[index - 1, close_col] today_close = row[close_col] today_vol = row[vol_col] current_val = last_val + (today_vol * (today_close - last_close) / last_close) else: current_val = row[vol_col] data.set_value(index, 'pvt', current_val) data['pvt_ema' + str(trend_periods)] = data['pvt'].ewm(ignore_na=False, min_periods=0, com=trend_periods, adjust=True).mean() return data"""Average true range (ATR)Source: https://en.wikipedia.org/wiki/Average_true_rangeParams: data: pandas DataFrame trend_periods: the over which to calculate ATR open_col: the name of the OPEN values column high_col: the name of the HIGH values column low_col: the name of the LOW values column close_col: the name of the CLOSE values column vol_col: the name of the VOL values column drop_tr: whether to drop the True Range values column from the resulting DataFrame Returns: copy of 'data' DataFrame with 'atr' (and 'true_range' if 'drop_tr' == True) column(s) added"""def average_true_range(data, trend_periods=14, open_col='&lt;OPEN&gt;', high_col='&lt;HIGH&gt;', low_col='&lt;LOW&gt;', close_col='&lt;CLOSE&gt;', drop_tr = True): for index, row in data.iterrows(): prices = [row[high_col], row[low_col], row[close_col], row[open_col]] if index &gt; 0: val1 = np.amax(prices) - np.amin(prices) val2 = abs(np.amax(prices) - data.at[index - 1, close_col]) val3 = abs(np.amin(prices) - data.at[index - 1, close_col]) true_range = np.amax([val1, val2, val3]) else: true_range = np.amax(prices) - np.amin(prices) data.set_value(index, 'true_range', true_range) data['atr'] = data['true_range'].ewm(ignore_na=False, min_periods=0, com=trend_periods, adjust=True).mean() if drop_tr: data = data.drop(['true_range'], axis=1) return data"""Bollinger BandsSource: https://en.wikipedia.org/wiki/Bollinger_BandsParams: data: pandas DataFrame trend_periods: the over which to calculate BB close_col: the name of the CLOSE values column Returns: copy of 'data' DataFrame with 'bol_bands_middle', 'bol_bands_upper' and 'bol_bands_lower' columns added"""def bollinger_bands(data, trend_periods=20, close_col='&lt;CLOSE&gt;'): data['bol_bands_middle'] = data[close_col].ewm(ignore_na=False, min_periods=0, com=trend_periods, adjust=True).mean() for index, row in data.iterrows(): s = data[close_col].iloc[index - trend_periods: index] sums = 0 middle_band = data.at[index, 'bol_bands_middle'] for e in s: sums += np.square(e - middle_band) std = np.sqrt(sums / trend_periods) d = 2 upper_band = middle_band + (d * std) lower_band = middle_band - (d * std) data.set_value(index, 'bol_bands_upper', upper_band) data.set_value(index, 'bol_bands_lower', lower_band) return data"""Chaikin OscillatorSource: http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:chaikin_oscillatorParams: data: pandas DataFrame periods_short: period for the shorter EMA (3 days recommended) periods_long: period for the longer EMA (10 days recommended) high_col: the name of the HIGH values column low_col: the name of the LOW values column close_col: the name of the CLOSE values column vol_col: the name of the VOL values column Returns: copy of 'data' DataFrame with 'ch_osc' column added"""def chaikin_oscillator(data, periods_short=3, periods_long=10, high_col='&lt;HIGH&gt;', low_col='&lt;LOW&gt;', close_col='&lt;CLOSE&gt;', vol_col='&lt;VOL&gt;'): ac = pd.Series([]) val_last = 0 for index, row in data.iterrows(): if row[high_col] != row[low_col]: val = val_last + ((row[close_col] - row[low_col]) - (row[high_col] - row[close_col])) / (row[high_col] - row[low_col]) * row[vol_col] else: val = val_last ac.set_value(index, val) val_last = val ema_long = ac.ewm(ignore_na=False, min_periods=0, com=periods_long, adjust=True).mean() ema_short = ac.ewm(ignore_na=False, min_periods=0, com=periods_short, adjust=True).mean() data['ch_osc'] = ema_short - ema_long return data"""Typical PriceSource: https://en.wikipedia.org/wiki/Typical_priceParams: data: pandas DataFrame high_col: the name of the HIGH values column low_col: the name of the LOW values column close_col: the name of the CLOSE values column Returns: copy of 'data' DataFrame with 'typical_price' column added"""def typical_price(data, high_col = '&lt;HIGH&gt;', low_col = '&lt;LOW&gt;', close_col = '&lt;CLOSE&gt;'): data['typical_price'] = (data[high_col] + data[low_col] + data[close_col]) / 3 return data"""Ease of MovementSource: http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:ease_of_movement_emvParams: data: pandas DataFrame period: period for calculating EMV high_col: the name of the HIGH values column low_col: the name of the LOW values column vol_col: the name of the VOL values column Returns: copy of 'data' DataFrame with 'emv' and 'emv_ema_[period]' columns added"""def ease_of_movement(data, period=14, high_col='&lt;HIGH&gt;', low_col='&lt;LOW&gt;', vol_col='&lt;VOL&gt;'): for index, row in data.iterrows(): if index &gt; 0: midpoint_move = (row[high_col] + row[low_col]) / 2 - (data.at[index - 1, high_col] + data.at[index - 1, low_col]) / 2 else: midpoint_move = 0 diff = row[high_col] - row[low_col] if diff == 0: #this is to avoid division by zero below diff = 0.000000001 vol = row[vol_col] if vol == 0: vol = 1 box_ratio = (vol / 100000000) / (diff) emv = midpoint_move / box_ratio data.set_value(index, 'emv', emv) data['emv_ema_'+str(period)] = data['emv'].ewm(ignore_na=False, min_periods=0, com=period, adjust=True).mean() return data"""Mass IndexSource: http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:mass_indexParams: data: pandas DataFrame period: period for calculating MI (9 days recommended) high_col: the name of the HIGH values column low_col: the name of the LOW values column Returns: copy of 'data' DataFrame with 'mass_index' column added"""def mass_index(data, period=25, ema_period=9, high_col='&lt;HIGH&gt;', low_col='&lt;LOW&gt;'): high_low = data[high_col] - data[low_col] + 0.000001 #this is to avoid division by zero below ema = high_low.ewm(ignore_na=False, min_periods=0, com=ema_period, adjust=True).mean() ema_ema = ema.ewm(ignore_na=False, min_periods=0, com=ema_period, adjust=True).mean() div = ema / ema_ema for index, row in data.iterrows(): if index &gt;= period: val = div[index-25:index].sum() else: val = 0 data.set_value(index, 'mass_index', val) return data"""Average directional movement indexSource: https://en.wikipedia.org/wiki/Average_directional_movement_indexParams: data: pandas DataFrame periods: period for calculating ADX (14 days recommended) high_col: the name of the HIGH values column low_col: the name of the LOW values column Returns: copy of 'data' DataFrame with 'adx', 'dxi', 'di_plus', 'di_minus' columns added"""def directional_movement_index(data, periods=14, high_col='&lt;HIGH&gt;', low_col='&lt;LOW&gt;'): remove_tr_col = False if not 'true_range' in data.columns: data = average_true_range(data, drop_tr = False) remove_tr_col = True data['m_plus'] = 0. data['m_minus'] = 0. for i,row in data.iterrows(): if i&gt;0: data.set_value(i, 'm_plus', row[high_col] - data.at[i-1, high_col]) data.set_value(i, 'm_minus', row[low_col] - data.at[i-1, low_col]) data['dm_plus'] = 0. data['dm_minus'] = 0. for i,row in data.iterrows(): if row['m_plus'] &gt; row['m_minus'] and row['m_plus'] &gt; 0: data.set_value(i, 'dm_plus', row['m_plus']) if row['m_minus'] &gt; row['m_plus'] and row['m_minus'] &gt; 0: data.set_value(i, 'dm_minus', row['m_minus']) data['di_plus'] = (data['dm_plus'] / data['true_range']).ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean() data['di_minus'] = (data['dm_minus'] / data['true_range']).ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean() data['dxi'] = np.abs(data['di_plus'] - data['di_minus']) / (data['di_plus'] + data['di_minus']) data.set_value(0, 'dxi',1.) data['adx'] = data['dxi'].ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean() data = data.drop(['m_plus', 'm_minus', 'dm_plus', 'dm_minus'], axis=1) if remove_tr_col: data = data.drop(['true_range'], axis=1) return data"""Money Flow Index (MFI)Source: http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:money_flow_index_mfiParams: data: pandas DataFrame periods: period for calculating MFI (14 days recommended) vol_col: the name of the VOL values column Returns: copy of 'data' DataFrame with 'money_flow_index' column added"""def money_flow_index(data, periods=14, vol_col='&lt;VOL&gt;'): remove_tp_col = False if not 'typical_price' in data.columns: data = typical_price(data) remove_tp_col = True data['money_flow'] = data['typical_price'] * data[vol_col] data['money_ratio'] = 0. data['money_flow_index'] = 0. data['money_flow_positive'] = 0. data['money_flow_negative'] = 0. for index,row in data.iterrows(): if index &gt; 0: if row['typical_price'] &lt; data.at[index-1, 'typical_price']: data.set_value(index, 'money_flow_positive', row['money_flow']) else: data.set_value(index, 'money_flow_negative', row['money_flow']) if index &gt;= periods: period_slice = data['money_flow'][index-periods:index] positive_sum = data['money_flow_positive'][index-periods:index].sum() negative_sum = data['money_flow_negative'][index-periods:index].sum() if negative_sum == 0.: #this is to avoid division by zero below negative_sum = 0.00001 m_r = positive_sum / negative_sum mfi = 1-(1 / (1 + m_r)) data.set_value(index, 'money_ratio', m_r) data.set_value(index, 'money_flow_index', mfi) data = data.drop(['money_flow', 'money_ratio', 'money_flow_positive', 'money_flow_negative'], axis=1) if remove_tp_col: data = data.drop(['typical_price'], axis=1) return data"""Negative Volume Index (NVI)Source: http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:negative_volume_indeParams: data: pandas DataFrame periods: period for calculating NVI (255 days recommended) close_col: the name of the CLOSE values column vol_col: the name of the VOL values column Returns: copy of 'data' DataFrame with 'nvi' and 'nvi_ema' columns added"""def negative_volume_index(data, periods=255, close_col='&lt;CLOSE&gt;', vol_col='&lt;VOL&gt;'): data['nvi'] = 0. for index,row in data.iterrows(): if index &gt; 0: prev_nvi = data.at[index-1, 'nvi'] prev_close = data.at[index-1, close_col] if row[vol_col] &lt; data.at[index-1, vol_col]: nvi = prev_nvi + (row[close_col] - prev_close / prev_close * prev_nvi) else: nvi = prev_nvi else: nvi = 1000 data.set_value(index, 'nvi', nvi) data['nvi_ema'] = data['nvi'].ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean() return data"""Positive Volume Index (PVI)Source: https://www.equities.com/news/the-secret-to-the-positive-volume-indexParams: data: pandas DataFrame periods: period for calculating PVI (255 days recommended) close_col: the name of the CLOSE values column vol_col: the name of the VOL values column Returns: copy of 'data' DataFrame with 'pvi' and 'pvi_ema' columns added"""def positive_volume_index(data, periods=255, close_col='&lt;CLOSE&gt;', vol_col='&lt;VOL&gt;'): data['pvi'] = 0. for index,row in data.iterrows(): if index &gt; 0: prev_pvi = data.at[index-1, 'pvi'] prev_close = data.at[index-1, close_col] if row[vol_col] &gt; data.at[index-1, vol_col]: pvi = prev_pvi + (row[close_col] - prev_close / prev_close * prev_pvi) else: pvi = prev_pvi else: pvi = 1000 data.set_value(index, 'pvi', pvi) data['pvi_ema'] = data['pvi'].ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean() return data"""MomentumSource: https://en.wikipedia.org/wiki/Momentum_(technical_analysis)Params: data: pandas DataFrame periods: period for calculating momentum close_col: the name of the CLOSE values column Returns: copy of 'data' DataFrame with 'momentum' column added"""def momentum(data, periods=14, close_col='&lt;CLOSE&gt;'): data['momentum'] = 0. for index,row in data.iterrows(): if index &gt;= periods: prev_close = data.at[index-periods, close_col] val_perc = (row[close_col] - prev_close)/prev_close data.set_value(index, 'momentum', val_perc) return data"""Relative Strenght IndexSource: https://en.wikipedia.org/wiki/Relative_strength_indexParams: data: pandas DataFrame periods: period for calculating momentum close_col: the name of the CLOSE values column Returns: copy of 'data' DataFrame with 'rsi' column added"""def rsi(data, periods=14, close_col='&lt;CLOSE&gt;'): data['rsi_u'] = 0. data['rsi_d'] = 0. data['rsi'] = 0. for index,row in data.iterrows(): if index &gt;= periods: prev_close = data.at[index-periods, close_col] if prev_close &lt; row[close_col]: data.set_value(index, 'rsi_u', row[close_col] - prev_close) elif prev_close &gt; row[close_col]: data.set_value(index, 'rsi_d', prev_close - row[close_col]) data['rsi'] = data['rsi_u'].ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean() / (data['rsi_u'].ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean() + data['rsi_d'].ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean()) data = data.drop(['rsi_u', 'rsi_d'], axis=1) return data"""Chaikin Volatility (CV)Source: https://www.marketvolume.com/technicalanalysis/chaikinvolatility.aspParams: data: pandas DataFrame ema_periods: period for smoothing Highest High and Lowest Low difference change_periods: the period for calculating the difference between Highest High and Lowest Low high_col: the name of the HIGH values column low_col: the name of the LOW values column close_col: the name of the CLOSE values column Returns: copy of 'data' DataFrame with 'chaikin_volatility' column added"""def chaikin_volatility(data, ema_periods=10, change_periods=10, high_col='&lt;HIGH&gt;', low_col='&lt;LOW&gt;', close_col='&lt;CLOSE&gt;'): data['ch_vol_hl'] = data[high_col] - data[low_col] data['ch_vol_ema'] = data['ch_vol_hl'].ewm(ignore_na=False, min_periods=0, com=ema_periods, adjust=True).mean() data['chaikin_volatility'] = 0. for index,row in data.iterrows(): if index &gt;= change_periods: prev_value = data.at[index-change_periods, 'ch_vol_ema'] if prev_value == 0: #this is to avoid division by zero below prev_value = 0.0001 data.set_value(index, 'chaikin_volatility', ((row['ch_vol_ema'] - prev_value)/prev_value)) data = data.drop(['ch_vol_hl', 'ch_vol_ema'], axis=1) return data"""William's Accumulation/DistributionSource: https://www.metastock.com/customer/resources/taaz/?p=125Params: data: pandas DataFrame high_col: the name of the HIGH values column low_col: the name of the LOW values column close_col: the name of the CLOSE values column Returns: copy of 'data' DataFrame with 'williams_ad' column added"""def williams_ad(data, high_col='&lt;HIGH&gt;', low_col='&lt;LOW&gt;', close_col='&lt;CLOSE&gt;'): data['williams_ad'] = 0. for index,row in data.iterrows(): if index &gt; 0: prev_value = data.at[index-1, 'williams_ad'] prev_close = data.at[index-1, close_col] if row[close_col] &gt; prev_close: ad = row[close_col] - min(prev_close, row[low_col]) elif row[close_col] &lt; prev_close: ad = row[close_col] - max(prev_close, row[high_col]) else: ad = 0. data.set_value(index, 'williams_ad', (ad+prev_value)) return data"""William's % RSource: https://www.metastock.com/customer/resources/taaz/?p=126Params: data: pandas DataFrame periods: the period over which to calculate the indicator value high_col: the name of the HIGH values column low_col: the name of the LOW values column close_col: the name of the CLOSE values column Returns: copy of 'data' DataFrame with 'williams_r' column added"""def williams_r(data, periods=14, high_col='&lt;HIGH&gt;', low_col='&lt;LOW&gt;', close_col='&lt;CLOSE&gt;'): data['williams_r'] = 0. for index,row in data.iterrows(): if index &gt; periods: data.set_value(index, 'williams_r', ((max(data[high_col][index-periods:index]) - row[close_col]) / (max(data[high_col][index-periods:index]) - min(data[low_col][index-periods:index])))) return data"""TRIXSource: https://www.metastock.com/customer/resources/taaz/?p=114Params: data: pandas DataFrame periods: the period over which to calculate the indicator value signal_periods: the period for signal moving average close_col: the name of the CLOSE values column Returns: copy of 'data' DataFrame with 'trix' and 'trix_signal' columns added"""def trix(data, periods=14, signal_periods=9, close_col='&lt;CLOSE&gt;'): data['trix'] = data[close_col].ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean() data['trix'] = data['trix'].ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean() data['trix'] = data['trix'].ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean() data['trix_signal'] = data['trix'].ewm(ignore_na=False, min_periods=0, com=signal_periods, adjust=True).mean() return data"""Ultimate OscillatorSource: http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:ultimate_oscillatorParams: data: pandas DataFrame period_1: the period of the first average (7 days recommended) period_2: the period of the second average (14 days recommended) period_3: the period of the third average (28 days recommended) high_col: the name of the HIGH values column low_col: the name of the LOW values column close_col: the name of the CLOSE values column Returns: copy of 'data' DataFrame with 'ultimate_oscillator' column added"""def ultimate_oscillator(data, period_1=7,period_2=14, period_3=28, high_col='&lt;HIGH&gt;', low_col='&lt;LOW&gt;', close_col='&lt;CLOSE&gt;'): data['ultimate_oscillator'] = 0. data['uo_bp'] = 0. data['uo_tr'] = 0. data['uo_avg_1'] = 0. data['uo_avg_2'] = 0. data['uo_avg_3'] = 0. for index,row in data.iterrows(): if index &gt; 0: bp = row[close_col] - min(row[low_col], data.at[index-1, close_col]) tr = max(row[high_col], data.at[index-1, close_col]) - min(row[low_col], data.at[index-1, close_col]) data.set_value(index, 'uo_bp', bp) data.set_value(index, 'uo_tr', tr) if index &gt;= period_1: uo_avg_1 = sum(data['uo_bp'][index-period_1:index]) / sum(data['uo_tr'][index-period_1:index]) data.set_value(index, 'uo_avg_1', uo_avg_1) if index &gt;= period_2: uo_avg_2 = sum(data['uo_bp'][index-period_2:index]) / sum(data['uo_tr'][index-period_2:index]) data.set_value(index, 'uo_avg_2', uo_avg_2) if index &gt;= period_3: uo_avg_3 = sum(data['uo_bp'][index-period_3:index]) / sum(data['uo_tr'][index-period_3:index]) data.set_value(index, 'uo_avg_3', uo_avg_3) uo = (4 * uo_avg_1 + 2 * uo_avg_2 + uo_avg_3) / 7 data.set_value(index, 'ultimate_oscillator', uo) data = data.drop(['uo_bp', 'uo_tr', 'uo_avg_1', 'uo_avg_2', 'uo_avg_3'], axis=1) return data]]></content>
      <categories>
        <category>programming/python/technical indicators</category>
      </categories>
      <tags>
        <tag>stock market</tag>
        <tag>technical indicators</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启发式算法的性能分析]]></title>
    <url>%2F2018%2F09%2F07%2F%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[评价算法优劣的指标 算法的复杂性（计算效率） 解的偏离程度（计算效果） 算法的稳健性（不同实例、不同时间、不同起点的差异）评价算法优劣的手段 最坏情况分析（纯理论） 概率分析（理论分析） 计算模拟分析（统计特性）]]></content>
      <categories>
        <category>math/algorithm</category>
      </categories>
      <tags>
        <tag>算法的评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[example iframe dash]]></title>
    <url>%2F2018%2F08%2F14%2Fexample-iframe-dash%2F</url>
    <content type="text"><![CDATA[//iframe 高度自适应 function setIframeHeight(iframe) { iframe.οnlοad=function(){ var iframeWin = iframe.contentWindow || iframe.contentDocument.parentWindow; var iframeHeight=iframeWin.document.documentElement.scrollHeight || iframeWin.document.body.scrollHeight; iframe.height = iframeHeight+10; } setIframeHeight(document.getElementById('sub_frame')); setIframeHeight(document.getElementById('sub_frame2')); http://127.0.0.1:8050/ example iframe dash 1&lt;iframe src=&quot;http://127.0.0.1:8050/&quot; id=&quot;sub_frame&quot; width=&quot;800&quot; height=&quot;800&quot;&gt;&lt;/iframe&gt;]]></content>
      <categories>
        <category>programming/JavaScript</category>
      </categories>
      <tags>
        <tag>HTML</tag>
        <tag>iframe</tag>
        <tag>嵌套网页</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django-plotly-dash文件结构]]></title>
    <url>%2F2018%2F04%2F18%2Fdjango-plotly-dash%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[django-plotly-dash文件结构 ./demo/ demo/ urls.py &lt;路由配置文件&gt; templates/ &lt;模板文件&gt; example.html django_plotly_dash/ django_plotly_dash.egg-info/ docs/]]></content>
      <categories>
        <category>programming/python/django-plotly-dash</category>
      </categories>
      <tags>
        <tag>django-plotly-dash</tag>
        <tag>dpd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正向激励和非均衡数据集、损失函数加权重]]></title>
    <url>%2F2017%2F08%2F20%2F%E6%AD%A3%E5%90%91%E6%BF%80%E5%8A%B1%E5%92%8C%E9%9D%9E%E5%9D%87%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8A%A0%E6%9D%83%E9%87%8D%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[行为经济学的八大原理]]></title>
    <url>%2F2017%2F08%2F17%2F%E8%A1%8C%E4%B8%BA%E7%BB%8F%E6%B5%8E%E5%AD%A6%E7%9A%84%E5%85%AB%E5%A4%A7%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>economics/behavioral economics</category>
      </categories>
      <tags>
        <tag>行为经济学的八大原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无约束优化问题的数值算法]]></title>
    <url>%2F2017%2F08%2F17%2F%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%95%B0%E5%80%BC%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[无约束优化问题的数值算法最速下降法Newton法共轭梯度法变尺度法直接法 坐标轮换法 步长加速法 方向加速法]]></content>
      <categories>
        <category>mant/algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[什么是钩子]]></title>
    <url>%2F2017%2F08%2F15%2F%E4%BB%80%E4%B9%88%E6%98%AF%E9%92%A9%E5%AD%90%2F</url>
    <content type="text"><![CDATA[对于Windows系统，它是建立在事件驱动机制上的，说白了就是整个系统都是通过消息传递实现的。hook（钩子）是一种特殊的消息处理机制，它可以监视系统或者进程中的各种事件消息，截获发往目标窗口的消息并进行处理。所以说，我们可以在系统中自定义钩子，用来监视系统中特定事件的发生，完成特定功能，如屏幕取词，监视日志，截获键盘、鼠标输入等等。 钩子的种类很多，每种钩子可以截获相应的消息，如键盘钩子可以截获键盘消息，外壳钩子可以截取、启动和关闭应用程序的消息等。钩子可以分为线程钩子和系统钩子，线程钩子可以监视指定线程的事件消息，系统钩子监视系统中的所有线程的事件消息。因为系统钩子会影响系统中所有的应用程序，所以钩子函数必须放在独立的动态链接库(DLL) 中。 所以说，hook（钩子）就是一个Windows消息的拦截机制，可以拦截单个进程的消息(线程钩子)，也可以拦截所有进程的消息(系统钩子)，也可以对拦截的消息进行自定义的处理。Windows消息带了一些程序有用的信息，比如Mouse类信息，就带有鼠标所在窗体句柄、鼠标位置等信息，拦截了这些消息，就可以做出例如金山词霸一类的屏幕取词功能。]]></content>
      <categories>
        <category>programming/nodejs</category>
      </categories>
      <tags>
        <tag>concept</tag>
        <tag>hook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从测度论视角理解傅里叶变换和音频降噪]]></title>
    <url>%2F2017%2F08%2F14%2F%E4%BB%8E%E6%B5%8B%E5%BA%A6%E8%AE%BA%E8%A7%86%E8%A7%92%E7%90%86%E8%A7%A3%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%92%8C%E9%9F%B3%E9%A2%91%E9%99%8D%E5%99%AA%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[量化交易dash框架]]></title>
    <url>%2F2017%2F08%2F13%2F%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93dash%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[python可视化 dash]]></title>
    <url>%2F2017%2F08%2F13%2Fpython%E5%8F%AF%E8%A7%86%E5%8C%96-dash%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[计量模型、机器学习名词表]]></title>
    <url>%2F2016%2F12%2F30%2F%E8%AE%A1%E9%87%8F%E6%A8%A1%E5%9E%8B%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%8D%E8%AF%8D%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[总纲支持向量机、核函数、决策树、随机森林、GBTs解决过拟合问题 线性回归 逻辑回归AUCROC 梯度下降 特征处理 多重共线性 内生性 支持向量机不会考虑所有数据，而只是关心很难被超平面分割的‘异常点’。SVR(support vector regression) 核函数低维空间映射到高维空间，非线性问题转换为高维空间里的线性问题。 决策树白盒模型，通过生成决策规则来解决分类和回归问题。sklearn tree DecisionTreeClassifier gini entropy max_depth max_leaf_nodes best-first search min_samples_split min_samples_leaf min_impurity_split对于分类问题：只考虑叶子节点里哪个分类最大对于回归问题：叶子节点里的加权平均值决策树里的惩罚项 树的集成模型的联结解决变量边际效应恒定的隐含假设1）特征 分箱（卡方检验） 01型变量 逻辑回归2）特征 决策树 叶子节点 逻辑回归 （适用于广告行业和金融反欺诈）3）特征 GBTs 叶子节点 逻辑回归gradient-boosted trees(GBTs)模型评估 ROC 2）利用了决策树的优点能综合考虑多个变量，而且对变量的线性变换是稳定的。叶子节点是类别型变量 剪枝pruning决策树模型的过拟合问题：决策树属于非参模型（nonparametric model),有无穷多个参数，更容易过拟合。pre-pruning: max_depth min_sample_splitpost_pruning: 减掉不纯度下降不明显的 消除误差剪枝法 Reduced Error Pruning REPcross validation: train set, test set, pruning set训练集、验证集、测试集训练集、验证集、测试集、剪枝集bottom-up restriction 树的集成(哲学层面：联结主义，具体算法：集成方法)weak learner 平均方法 averaging methods随机森林假设决策树是相互独立的如果每颗决策树是一样的，那么随机森林等同于决策树模型 决策树的随机性1）随机选取训练决策树的数据2）随机选取候选自变量3）随机选取阈值集合sklearn:random forests 1) 2) RandomForestClassifier 分类问题 RandomForestRegressor 回归问题extremely randomized tresss 1) 2) 3) ExtraTreesClassifier ExtraTreesRegressor Random forest embedding(非监督学习)几乎所有的监督学习都可以作为非监督学习使用低维数据到高维数据：支持向量机的核函数、随机森林的random forest embedding 提升方法 boosting methodsGBTs(gradient boosted trees)梯度提升决策树损失函数MSE因为决策树本身并没有明确的模型参数，而无法使用梯度下降法把决策树本身抽象成整体模型的一个参数，并使用梯度下降的方法对其进行更新。GBTs模型超参数深度m为了防止过拟合，引入了学习速率这个超参数 vsklearn GradientBoostingRegressor GradientBoostingClassifier 生成式模型判别式模型（discriminative model）生成式模型 (generative model) 贝叶斯框架蒙题霍尔问题先验概率、后验概率先验概率知因求果，后验概率知果求因 贝叶斯学派与频率学派 Bayesian Frequentist频率学派：随机性真实存在，能被适合的模型所捕捉，模型的参数本身是确定的值，参数的估计值是一个随机变量。通过不断地调整模型的functional form来逼近真实模型。贝叶斯学派：参数不再是确定的值，而是随机数$ P(a,b,\sigma | y,x) \sim P(y|x,a,b, \sigma)P(a)P(b)P(\sigma)$Bayesian linear regression一种更加一般的线性回归模型，与普通的回归模型相比，此模型的参数估计值更加靠近0。当参数的先验分布为特定分布时，此模型就是之前讨论过的岭回归，该模型可以理解为在线性回归的基础上加上了惩罚项。根据参数的后验分布来得到估计值数据里的随机性更多的来自于参数本身的随机分布。贝叶斯学派认为参数的分布情况反映了观察者对事物的认识并不完全。（我：贝叶斯学派可以囊括频率学派）。观察者不断根据信息，来更新自己的知识，最终根据新的知识得到参数的估计值。 朴素贝叶斯特征提取，停止词stop words8]]></content>
      <categories>
        <category>programming/python/machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>名词表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习的共识]]></title>
    <url>%2F2016%2F11%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%85%B1%E8%AF%86%2F</url>
    <content type="text"><![CDATA[数据和特征决定了机器学习的上限，而模型和算法在逼近这个上限。 模型在本质上是训练数据的一种线性组合。 算法的思路：寻找与被预测数据相似的训练数据，并将相应的因变量加权平均得到最后的预测值。]]></content>
      <categories>
        <category>programming/python/machine learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>共识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch Gradient Descent Mini-Batch Gradient Descent and Stochastic Gradient Descent]]></title>
    <url>%2F2016%2F10%2F19%2FBatch-Gradient-Descent-Mini-Batch-Gradient-Descent-and-Stochastic-Gradient-Descent%2F</url>
    <content type="text"><![CDATA[batch gradient descent理想状态下经过足够多的迭代后可以达到全局最优。很难处理大数据集 123456789101112X = data_inputY = labelsparameters = initialize_parameters(layers_dims)for i in range(0, num_iterations): #num_iterations--迭代次数 # Forward propagation a, caches = forward_propagation(X, parameters) # Compute cost. cost = compute_cost(a, Y) # Backward propagation. grads = backward_propagation(a, caches, parameters) # Update parameters. parameters = update_parameters(parameters, grads) stochastic gradient descent每次也可以训练一小批样本随机性避免陷入局部最优，但是每次更新方向会在总体的梯度向量方向上震荡。和遗传算法有点像 123456789101112131415X = data_inputY = labelspermutation = list(np.random.permutation(m))shuffled_X = X[:, permutation]shuffled_Y = Y[:, permutation].reshape((1, m))for i in range(0, num_iterations): for j in range(0, m): # 每次训练一个样本 # Forward propagation AL,caches = forward_propagation(shuffled_X[:, j].reshape(-1,1), parameters) # Compute cost cost = compute_cost(AL, shuffled_Y[:, j].reshape(1,1)) # Backward propagation grads = backward_propagation(AL, shuffled_Y[:,j].reshape(1,1), caches) # Update parameters. parameters = update_parameters(parameters, grads, learning_rate) Mini-batch gradient descentbatch_size =m = 2 ** n深度学习基本都用 Mini-batch gradient descentSGD = stochastic mini-batch gradient descent 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: random_mini_batchesdef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0): """ Creates a list of random minibatches from (X, Y) Arguments: X -- input data, of shape (input size, number of examples) Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples) mini_batch_size -- size of the mini-batches, integer Returns: mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y) """ np.random.seed(seed) # To make your "random" minibatches the same as ours m = X.shape[1] # number of training examples mini_batches = [] # Step 1: Shuffle (X, Y) permutation = list(np.random.permutation(m)) shuffled_X = X[:, permutation] shuffled_Y = Y[:, permutation].reshape((1,m)) # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case. num_complete_minibatches = m//mini_batch_size # number of mini batches for k in range(0, num_complete_minibatches): mini_batch_X = shuffled_X[:, k * mini_batch_size: (k + 1) * mini_batch_size] mini_batch_Y = shuffled_Y[:, k * mini_batch_size: (k + 1) * mini_batch_size] mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) # Handling the end case (last mini-batch &lt; mini_batch_size) if m % mini_batch_size != 0: mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m] mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m] mini_batch = (mini_batch_X, mini_batch_Y) mini_batches.append(mini_batch) return mini_batches 123456789101112131415seed = 0for i in range(0, num_iterations): # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch seed = seed + 1 minibatches = random_mini_batches(X, Y, mini_batch_size, seed) for minibatch in minibatches: # Select a minibatch (minibatch_X, minibatch_Y) = minibatch # Forward propagation AL, caches = forward_propagation(minibatch_X, parameters) # Compute cost cost = compute_cost(AL, minibatch_Y) # Backward propagation grads = backward_propagation(AL, minibatch_Y, caches) parameters = update_parameters(parameters, grads, learning_rate) 超参数batch_size]]></content>
      <categories>
        <category>programming/python/gradient descent</category>
      </categories>
      <tags>
        <tag>batch gradient descent</tag>
        <tag>mini-batch gradient descend</tag>
        <tag>stochastic gradient descent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.pipeline]]></title>
    <url>%2F2016%2F10%2F11%2Fsklearn-pipeline%2F</url>
    <content type="text"><![CDATA[polynominal regression 只在x样本的大小范围内有效。 1234567891011121314151617181920212223242526272829303132333435363738394041424344""" demo sklearn.pipeline"""import numpy as npimport matplotlib.pyplot as pltimport sklearn.linear_model as lmimport sklearn.preprocessing as spimport sklearn.metrics as smimport sklearn.pipeline as pl# 采集数据x, y = np.loadtxt( '../ml_data/single.txt', delimiter=',', usecols=(0, 1), unpack=True)# 训练多项式回归模型x = x.reshape(-1, 1)model = pl.make_pipeline( sp.PolynomialFeatures(8), lm.LinearRegression())# 训练模型model.fit(x, y)# 预测输出pred_y = model.predict(x)print(sm.r2_score(y, pred_y))print(sm.r2_score(pred_y, y))# 绘制多项式曲线px = np.linspace(x.min(), x.max(), 1000)pred_y = model.predict(px.reshape(-1, 1))# 画图plt.figure('Poly Regression', facecolor='lightgray')plt.title('Poly Regression', fontsize=16)plt.xlabel('X')plt.ylabel('Y')plt.tick_params(labelsize=10)plt.grid(linestyle=':')plt.scatter(x, y, color='dodgerblue', label='Samples', s=70, marker='o')plt.plot(px, pred_y, color='orangered', label='Poly Line')plt.legend()plt.savefig('polynominal.png')plt.show()]]></content>
      <categories>
        <category>programming/python/sklearn</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>pipeline</tag>
        <tag>pl</tag>
        <tag>polynominal regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性整流函数ReLU]]></title>
    <url>%2F2016%2F10%2F11%2F%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0ReLU%2F</url>
    <content type="text"><![CDATA[$f(x) = max(0,x)$ 在SVM中$f(x) = max(0, 1-y_i(w*X_i + c)$]]></content>
      <categories>
        <category>programming/python/ReLU</category>
      </categories>
      <tags>
        <tag>线性整流函数</tag>
        <tag>ReLU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn.linear_model]]></title>
    <url>%2F2016%2F09%2F30%2Fsklearn-linear-model%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435""" demo sklearn.linear_model"""import numpy as npimport matplotlib.pyplot as pltimport sklearn.linear_model as lmx, y = np.loadtxt( '../ml_data/single.txt', delimiter=',', usecols=(0, 1), unpack=True)# 选择、创建模型model = lm.LinearRegression()x = x.reshape(-1, 1) # 把x改为n行1列的2维数组model.fit(x, y)pred_y = model.predict(x)# 评估模型误差import sklearn.metrics as smprint(sm.mean_absolute_error(pred_y, y))print(sm.mean_squared_error(pred_y, y))print(sm.median_absolute_error(pred_y, y))print(sm.r2_score(pred_y, y))# 画图plt.figure('Linear Regression', facecolor='lightgray')plt.title('Linear Regression', fontsize=16)plt.xlabel('x')plt.ylabel('y')plt.tick_params(labelsize=10)plt.grid(linestyle=':')plt.scatter(x, y, color='dodgerblue', label='Samples', s=70, marker='o')plt.plot(x, pred_y, c='orangered', label='Regression')plt.legend()plt.savefig('sklearn-linear-regression.png')plt.show()]]></content>
      <categories>
        <category>programming/python/sklearn</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>linear_model</tag>
        <tag>lm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear Regression with Batch Gradient Descent]]></title>
    <url>%2F2016%2F09%2F28%2FLinear-Regression-with-Batch-Gradient-Descent%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107""" demo01_lr.py linear regression"""import numpy as npimport matplotlib.pyplot as pltimport matplotlib.pyplot as mpfrom mpl_toolkits.mplot3d import axes3dtrain_x = np.array([0.5, 0.6, 0.8, 1.1, 1.4])train_y = np.array([5.0, 5.5, 6.0, 6.8, 7.0])test_x = np.array([0.45, 0.55, 1.0, 1.3, 1.5])test_y = np.array([4.8, 5.3, 6.4, 6.9, 7.3])times = 1000 # 定义梯度下降次数lrate = 0.01 # 记录每次梯度下降参数变化率epoches = [] # 记录每次梯度下降的索引w0, w1, losses = [1], [1], []for i in range(1, times + 1): epoches.append(i) # 损失函数值 loss = (((w0[-1] + w1[-1] * train_x) - train_y) ** 2).sum() / 2 losses.append(loss) # 计算梯度 d0 = ((w0[-1] + w1[-1] * train_x) - train_y).sum() d1 = (((w0[-1] + w1[-1] * train_x) - train_y) * train_x).sum() # print('&#123;:4&#125;&gt; w0=&#123;:.8f&#125;, w1=&#123;:.8f&#125;, loss=&#123;:.8f&#125;'.format(epoches[-1], w0[-1], w1[-1], losses[-1])) w0.append(w0[-1] - lrate * d0) w1.append(w1[-1] - lrate * d1)# 通过最优的w0和w1,求出所有样本x的预测值ypred_y = w0[-1] + w1[-1] * train_xplt.figure('Linear Regression', facecolor='lightgray')plt.title('Linear Regression', fontsize=20)plt.xlabel('x', fontsize=14)plt.ylabel('y', fontsize=14)plt.tick_params(labelsize=10)plt.grid(linestyle=':')plt.scatter(train_x, train_y, marker='s', c='dodgerblue', alpha=0.5, s=80, label='Sample')plt.plot(train_x, pred_y, marker='D', c='orangered', alpha=0.5, label='Regression Line', linewidth=2)plt.legend()plt.savefig('sample-and-prediction')# 绘制w0 w1 loss的变化曲线图mp.figure('Training Progress', facecolor='lightgray')mp.title('Training Progress', fontsize=16)# w0mp.subplot(311)mp.ylabel('w0', fontsize=14)mp.grid(linestyle=':')mp.plot(epoches, w0[:-1], color='dodgerblue', label='w0')mp.legend()# w1mp.subplot(312)mp.ylabel('w1', fontsize=14)mp.grid(linestyle=':')mp.plot(epoches, w1[:-1], color='dodgerblue', label='w1')mp.legend()# lossmp.subplot(313)mp.ylabel('loss', fontsize=14)mp.grid(linestyle=':')mp.plot(epoches, losses, color='orangered', label='loss')mp.legend()mp.tight_layout()plt.savefig('weight-and-loss.png')# 基于三维曲面绘制梯度下降的过程中的每个散点n = 500w0_grid, w1_grid = np.meshgrid(np.linspace(0, 9, n), np.linspace(0, 3.5, n))loss = np.zeros_like(w0_grid)for x, y in zip(train_x, train_y): loss += (w0_grid + w1_grid * x - y)**2 / 2mp.figure('Loss Function', facecolor='lightgray')ax3d = mp.gca(projection='3d')ax3d.set_xlabel('w0', fontsize=14)ax3d.set_ylabel('w1', fontsize=14)ax3d.set_zlabel('loss', fontsize=14)ax3d.plot_surface(w0_grid, w1_grid, loss, cmap='jet')ax3d.plot(w0[:-1], w1[:-1], losses, 'o-', color='red')# mp.tight_layout()plt.savefig('gradient3D.png')# 以等高线的方式绘制梯度下降的过程。mp.figure('Batch Gradient Descent', facecolor='lightgray')mp.title('Batch Gradient Descent', fontsize=20)mp.xlabel('w0', fontsize=14)mp.ylabel('w1', fontsize=14)mp.tick_params(labelsize=10)mp.grid(linestyle=':')mp.contourf(w0_grid, w1_grid, loss, 10, cmap='jet')cntr = mp.contour(w0_grid, w1_grid, loss, 10, colors='black', linewidths=0.5)mp.clabel(cntr, inline_spacing=0.1, fmt='%.2f', fontsize=8)mp.plot(w0, w1, 'o-', c='orangered', label='BGD')mp.legend()plt.savefig('gradient-contour.png')plt.show()]]></content>
      <categories>
        <category>programming/python/sklearn</category>
      </categories>
      <tags>
        <tag>batch gradient descent</tag>
        <tag>linear regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx配置]]></title>
    <url>%2F2016%2F09%2F27%2Fnginx%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[brew install nginxvim /usr/local/etc/nginx/nginx.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129user root owner; # 设置访问权限#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 8080; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\.ht &#123; # deny all; #&#125; &#125; # 自定义端口和映射本地网站路径 server&#123; listen 5188; # 自定义端口 server_name localhost; location / &#123; root /Users/apple/dash-presentation/django-plotly-dash/frontend/public; # 本地文件夹路径 index index.html ; #设置默认网页 &#125; &#125; # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; include servers/*;&#125; 给予它管理员权限注意1.2.6是nginx版本，每个人的都不一样，记得修改。sudo chown root:wheel /usr/local/Cellar/nginx/1.2.6/bin/nginxsudo chmod u+s /usr/local/Cellar/nginx/1.2.6/bin/nginx nginx -s reloadsudo nginx -s stop 5188 http://localhost:5188/]]></content>
      <categories>
        <category>programming/nginx</category>
      </categories>
      <tags>
        <tag>nginx配置</tag>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R square]]></title>
    <url>%2F2016%2F09%2F26%2FR-square%2F</url>
    <content type="text"><![CDATA[$ R^2 = 1 - \frac{ (y - \widehat{y} )^2 }{ (y - \overline{y})^2 }$]]></content>
      <categories>
        <category>economics/economitrics</category>
      </categories>
      <tags>
        <tag>R square</tag>
        <tag>statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一维搜索算法分类]]></title>
    <url>%2F2016%2F09%2F15%2F%E4%B8%80%E7%BB%B4%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[试探法 成功失败法 Fibonacci法 黄金分割法切线法： 一维牛顿法抛物线法： 二次插值法]]></content>
      <categories>
        <category>math/algorithm</category>
      </categories>
      <tags>
        <tag>一维搜索算法</tag>
        <tag>concept</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优化问题的分类]]></title>
    <url>%2F2016%2F09%2F13%2F%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[函数优化问题组合优化问题迄今为止，许多组合优化问题都没有找到求最优解的多项式时间算法。]]></content>
      <categories>
        <category>math/convex analysis</category>
      </categories>
      <tags>
        <tag>优化问题的分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据维度转换]]></title>
    <url>%2F2016%2F09%2F01%2F%E6%95%B0%E6%8D%AE%E7%BB%B4%E5%BA%A6%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[数据升维从低维到高维SVM的核函数 kernel method决策树random trees embedding 数据降维除去随机因素的干扰避免过拟合降低工程实现难度方法： 线性判别分析 主成分分析PCA]]></content>
      <categories>
        <category>programming/python/machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>数据维度转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黄金分割法]]></title>
    <url>%2F2016%2F09%2F01%2F%E9%BB%84%E9%87%91%E5%88%86%E5%89%B2%E6%B3%95%2F</url>
    <content type="text"><![CDATA[黄金分割法(0.618法)适用于任何单峰值函数𝜙(𝑡)求极小点的问题，甚至对函数可以不要求连续。 单峰函数设𝜙 : [𝑎, 𝑏] ⊂ R → R，𝑡⋆是在[𝑎, 𝑏]上的全局极小点，如果对于[𝑎, 𝑏]上的任意两点𝑡1, 𝑡2，且𝑡1 &lt; 𝑡2都有𝜙(t1) &gt; 𝜙(t2), if t2 &lt; t*𝜙(t1) &lt; 𝜙(t2), if t1 &gt; t*那么称𝜙(𝑡)是区间[𝑎, 𝑏]上的单峰函数。若𝑡1 &lt; 𝑡⋆ &lt; 𝑡2，称[𝑡1, 𝑡2]为搜索区间。 算法步骤：步骤1 确定𝜙(𝑡)的初始搜索区间[𝑎, 𝑏]。步骤2 计算𝑡2 = 𝑎 + 𝛽(𝑏 − 𝑎)，𝜙2 = 𝜙(𝑡2)。步骤3 计算𝑡1 = 𝑎 + 𝛼(𝑏 − 𝑎)，𝜙1 = 𝜙(𝑡1)，。步骤4 若𝑡2-𝑡1≤ 𝜀，停机；否则，转步骤5。步骤5 若𝜙(𝑡1) ≤ 𝜙(𝑡2)，则𝑏 = 𝑡2, 𝑡2 = 𝑡1, 𝜙(𝑡2) = 𝜙(𝑡1), 𝑡1 = 𝑎 + 𝛼(𝑏 − 𝑎), 𝜙1 = 𝜙(𝑡1);否则，𝑎 = 𝑡1, 𝑡1 = 𝑡2, 𝜙(𝑡1) = 𝜙(𝑡2), 𝑡2 = 𝑎 + 𝛽(𝑏 − 𝑎), 𝜙2 = 𝜙(𝑡2).然后转步骤4。 123456789101112131415161718192021222324252627282930313233343536373839import mathfrom scipy.optimize import fmin,fminbound# one dimension golden searchdef f ( x ): L = math.exp(-x)+x**2 return Ldef golden(f ,*args): if not('a' in dir()): a=[] b=[] a.append(args[0]) b.append(args[1]) L=1e-16 # tolrence for convergence n=80 # max steps for iteration #a,b is the region containing opt point lambda1=a[0]+0.382*(b[0]-a[0]) miu1=a[0]+0.618*(b[0]-a[0]) #lambda1 miu1 is the test point of golden search method for k in range(0,n): if abs(b[k]-a[k])&lt;=L: solve=(a[k]+b[k])/2 break f_lambda1=f(lambda1) f_miu1=f(miu1) if f_lambda1&gt;f_miu1: a.append(lambda1) b.append(b[k]) lambda2=miu1 miu2=a[k+1]+0.618*(b[k+1]-a[k+1]) else: a.append(a[k]) b.append(miu1) miu2=lambda1 lambda2=a[k+1]+0.382*(b[k+1]-a[k+1]) lambda1=lambda2 miu1=miu2 print('optimum point is ：',solve) return solveprint(golden(f,0,1))]]></content>
      <categories>
        <category>math/algorithm/一维搜索方法</category>
      </categories>
      <tags>
        <tag>黄金分割法</tag>
        <tag>搜索算法</tag>
        <tag>一维搜索方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能优化算法及其应用]]></title>
    <url>%2F2016%2F08%2F27%2F%E6%99%BA%E8%83%BD%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[智能优化算法及其应用王凌清华大学出版社 现代优化计算方法邢文训清华大学出版社https://pan.baidu.com/s/129_FGFeLw3AxMZONBk_G8Q]]></content>
      <categories>
        <category>math/algorithm</category>
      </categories>
      <tags>
        <tag>课本</tag>
        <tag>Intelligent Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习和遗传算法的关系]]></title>
    <url>%2F2016%2F08%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[输入x 预期输出y的功能函数f 评估体系 fit 父代x 子代 x+1 评估体系 fit 能否通过对样本的学习来求解最优化？从3维到4维的求解样本(包括世世代代) 预测的最优解 评估函数 x(偏导、参数) 最优解]]></content>
  </entry>
  <entry>
    <title><![CDATA[numpy.npv净现值]]></title>
    <url>%2F2016%2F08%2F22%2Fnumpy-npv%E5%87%80%E7%8E%B0%E5%80%BC%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as np# 终值 = np.fv(利率, 期数, 每期支付, 现值)# 将1000元以1%的年利率存入银行5年，每年加存100元，# 到期后本息合计多少钱？fv = np.fv(0.01, 5, -100, -1000)print(round(fv, 2))# 现值 = np.pv(利率, 期数, 每期支付, 终值)# 将多少钱以1%的年利率存入银行5年，每年加存100元，# 到期后本息合计fv元？pv = np.pv(0.01, 5, -100, fv)print(pv)# 净现值 = np.npv(利率, 现金流)# 将1000元以1%的年利率存入银行5年，每年加存100元，# 相当于一次性存入多少钱？npv = np.npv(0.01, [ -1000, -100, -100, -100, -100, -100])print(round(npv, 2))fv = np.fv(0.01, 5, 0, npv)print(round(fv, 2))# 内部收益率 = np.irr(现金流)# 将1000元存入银行5年，以后逐年提现100元、200元、# 300元、400元、500元，银行利率达到多少，可在最后# 一次提现后偿清全部本息，即净现值为0元？irr = np.irr([-1000, 100, 200, 300, 400, 500])print(round(irr, 2))npv = np.npv(irr, [-1000, 100, 200, 300, 400, 500])print(npv)# 每期支付 = np.pmt(利率, 期数, 现值)# 以1%的年利率从银行贷款1000元，分5年还清，# 平均每年还多少钱？# paymentpmt = np.pmt(0.01, 5, 1000)print(round(pmt, 2))# 期数 = np.nper(利率, 每期支付, 现值)# 以1%的年利率从银行贷款1000元，平均每年还pmt元，# 多少年还清？nper = np.nper(0.01, pmt, 1000)print(int(nper))# 利率 = np.rate(期数, 每期支付, 现值, 终值)# 从银行贷款1000元，平均每年还pmt元，nper年还清，# 年利率多少？rate = np.rate(nper, pmt, 1000, 0)print(round(rate, 2))]]></content>
      <categories>
        <category>programming/python/numpy</category>
      </categories>
      <tags>
        <tag>numpy</tag>
        <tag>npv</tag>
        <tag>fv</tag>
        <tag>pv</tag>
        <tag>irr</tag>
        <tag>pmt</tag>
        <tag>nper</tag>
        <tag>rate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exponentially Weighted Windows]]></title>
    <url>%2F2016%2F08%2F20%2FExponentially-Weighted-Windows%2F</url>
    <content type="text"><![CDATA[指数加权的移动平均，移动方差，标准差，移动相关系数，移动协方差 理解移动平均值法的思路 移动平均法是一种简单平滑预测技术。消除随机波动的影响，看到的趋势的变化。 （我的思考）移动统计值可以看成是对时间序列的良好描述。移动统计值与普通的统计值相对应，可以看成是一小段时间上的统计值。我们有时候不单单需要知道预测值的大小，有时还需要对预测值的波动率进行预测，对预测值与预测值之间的关系进行预测。我认为预测的下一期的移动标准差和移动协方差就是很好的指标。联系：与volatility clusting的关系？ 推测使用机器学习来分析这些features对预测结果会产生有益的影响。 Exponentially Weighted Windows计算公式见 pandas/computation.htmlhttps://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#exponentially-weighted-windows]]></content>
      <categories>
        <category>programming/python/pandas</category>
      </categories>
      <tags>
        <tag>Expoentially Weighted Windows</tag>
        <tag>EWW</tag>
        <tag>EW moving average</tag>
        <tag>EW moving variance</tag>
        <tag>EW moving standard deviation</tag>
        <tag>EW moving correlation</tag>
        <tag>EW moving covariance</tag>
        <tag>时间序列</tag>
        <tag>volatility clusting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[qaundl dataset codes]]></title>
    <url>%2F2016%2F08%2F16%2Fqaundl-dataset-codes%2F</url>
    <content type="text"><![CDATA[Quandl CodesTo download a dataset, you will need to know its “Quandl code”. In the above example, you downloaded a dataset with the Quandl code “WIKI/FB”. Every Quandl code has 2 parts: the database code (“WIKI”) which specifies where the data comes from, and the dataset code (“FB”) which identifies the specific time series you want. You can find Quandl codes on our website, using our data browser.https://www.quandl.com/search]]></content>
      <categories>
        <category>programming/python/quandl</category>
      </categories>
      <tags>
        <tag>quandl</tag>
        <tag>get</tag>
        <tag>api</tag>
        <tag>dataset codes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[quandl.get]]></title>
    <url>%2F2016%2F08%2F16%2Fquandl-get%2F</url>
    <content type="text"><![CDATA[Make a time-series callThis call gets the WTI Crude Oil Price, which has a Quandl Code of EIA/PET_RWTC_D from the US Department of Energy dataset: data = quandl.get(“EIA/PET_RWTC_D”) Change formatsYou can get the same data in a NumPy array: data = quandl.get(“EIA/PET_RWTC_D”, returns=”numpy”) Make a filtered time-series callTo set start and end dates: data = quandl.get(“FRED/GDP”, start_date=”2001-12-31”, end_date=”2005-12-31”) To request specific columns: data = quandl.get([“NSE/OIL.1”, “WIKI/AAPL.4”]) To request the last 5 rows: data = quandl.get(“WIKI/AAPL”, rows=5) Preprocess the dataTo change the sampling frequency: data = quandl.get(“EIA/PET_RWTC_D”, collapse=”monthly”) To perform elementary calculations on the data: data = quandl.get(“FRED/GDP”, transformation=”rdiff”) Download an entire time-series datasetAn entire time-series dataset’s data can be downloaded. For example, to download the dataset ZEA: quandl.bulkdownload(“ZEA”) This call will download an entire time-series dataset as a ZIP file.]]></content>
      <categories>
        <category>programming/python/quandl</category>
      </categories>
      <tags>
        <tag>quandl</tag>
        <tag>get</tag>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库的schema]]></title>
    <url>%2F2016%2F08%2F15%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84schema%2F</url>
    <content type="text"><![CDATA[数据库的初学者往往会对关系型数据库模式（schema）、数据库（database）、表（table）、用户（user）之间感到迷惘，总感觉他们的关系千丝万缕，但又不知道他们的联系和区别在哪里，对一些问题往往说不出个所以然来。下面，我们就以SQL Server为核心，对其模式（schema）、数据库（database）、表（table）、用户（user）之间的关系展开讨论。 首先，我们先弄清楚什么是模式。 先明确一点，SQL Server中模式（schema）这个概念是在2005的版本里才提出来的，因此SQL Server2000不支持模式这个概念（本人曾在此处吃过亏）。 模式又称架构，架构的定义是形成单个命名空间的数据库实体的集合。命名空间是一个集合，其中每个元素的名称都是唯一的。在这里，我们可以将架构看成一个存放数据库中对象的一个容器。 上面的文字描述过于晦涩，举个简单的例子，平时要在电脑硬盘存放东西时，我们不会把所有的东西都存在一个文件夹里，而是会把不同的文件按照某一个标准分门别类，放到不同的文件夹里。而在数据库中，起到这个作用的就是架构，数据库对象（表、视图、存储过程，触发器等）按照一定的标准，存放在不同的架构里。有过java编程经验的同学都知道，命名空间名其实就是文件夹名，因此我们非常明确一点：一个对象只能属于一个架构，就像一个文件只能存放于一个文件夹中一样。与文件夹不同的是，架构是不能嵌套的，如此而已。因此，架构的好处非常明显——便于管理。 那么，现在我们来看看用户和模式（schema，即架构）有什么关系。 通过上面的分析，我们知道，一个架构可以容纳多个数据库对象，但并不是所有的用户都能访问某一个架构里的内容的，这就是所谓的权限。看下面一张表： 通过这张表，我们可以看出，用户1可以访问架构1和架构3，用户2可以访问架构1和架构2，以此类推。 在sql server2000中，用户和架构是不分离的，到了2005才分离。其实2000中的用户和架构概念就是为用户分配固定的模式，即如下表： 综合上面所述，用户和构架的关系是多对多的——一个架构可以对应多个用户，一个用户也可以对应多个架构。 现在，我们来讨论一下，数据库（database）和模式（schema）有什么关系。 举个很浅显的例子，我们可以把数据库看作是一个大仓库，仓库分了很多很多的房间，Schema就是其中的房间，一个Schema代表一个房间，于是乎，在不同的房间里，我们可以放不同的东西——有的放食物，有的放衣物……而这些不同的东西，就对应着我们数据库里的对象。 因此，我们可以看到，数据库与模式时一对多的关系。 总结一下，其实我们的数据库就是一个数据的大仓库，而里面创建了很多很多模式，分别放着不同的数据库对象（包括表），而不同的模式有不同的权限，于是，不同的用户就有不同的访问权限来访问某个模式里的数据库对象https://www.cnblogs.com/Neo-ds/p/4790413.html]]></content>
      <categories>
        <category>programming/mysql</category>
      </categories>
      <tags>
        <tag>programming</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot:dates]]></title>
    <url>%2F2016%2F08%2F12%2Fmatplotlib-pyplot-dates%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152""" demo12_loadtxt.py aapl.csv文件读取"""import numpy as npimport datetime as dtimport matplotlib.pyplot as pltimport matplotlib.dates as mddef dmy2ymd(dmy): dmy = str(dmy, encoding='utf-8') time = dt.datetime.strptime(dmy, '%d-%m-%Y').date() t = time.strftime('%Y-%m-%d') return t# dtype中的dates为M8[D], &lt;class 'numpy.ndarray'&gt;dates, opening_prices, highest_prices,\ lowest_prices, closing_prices = np.loadtxt( '../data/aapl.csv', delimiter=',', usecols=(1, 3, 4, 5, 6), unpack=True, dtype='M8[D], f8, f8, f8, f8', converters=&#123;1: dmy2ymd&#125; # 转换器函数)# 绘制收盘价折线图plt.figure('AAPL K', facecolor='lightgray')plt.title('AAPL K', fontsize=16)plt.xlabel('Day', fontsize=14)plt.ylabel('Price', fontsize=14)plt.tick_params(labelsize=10)plt.grid(linestyle=':')# 为了日期显示合理，修改dates的dtypedates = dates.astype(md.datetime.datetime)# 设置x轴刻度定位器ax = plt.gca()#设置主刻度定位器为周定位器（每周一显示主刻度文本）ax.xaxis.set_major_locator(md.WeekdayLocator(byweekday=md.MO))ax.xaxis.set_major_formatter(md.DateFormatter('%d %b %Y'))#设置次刻度定位器为日定位器ax.xaxis.set_minor_locator(md.DayLocator())plt.tick_params(labelsize=8)plt.plot(dates, closing_prices, color='dodgerblue', linewidth=2, linestyle='--', label='AAPL CP')plt.legend()plt.gcf().autofmt_xdate()plt.savefig('Apple_stock_prices_with_dates.png')plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pyplot</tag>
        <tag>dates</tag>
        <tag>loadtxt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.FuncAnimation:data_generator]]></title>
    <url>%2F2016%2F08%2F12%2Fmatplotlib-pyplot-FuncAnimation-data-generator%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npimport matplotlib.pyplot as pltimport matplotlib.animation as maplt.figure("Signal", facecolor='lightgray')plt.title("Signal", fontsize=14)plt.xlim(0, 10)plt.ylim(-3, 3)plt.grid(linestyle='--', color='lightgray', alpha=0.5)pl = plt.plot([], [], color='dodgerblue', label='Signal')[0]pl.set_data([],[])x = 0def y_generator(): global x while True: y = np.sin(2 * np.pi * x) * np.exp(np.sin(0.2 * np.pi * x)) yield (x, y) x += 0.05data = y_generator()t, v = [], []def update(i): print(i) global data x, y = next(data) t.append(x) v.append(y) #重新设置数据源 pl.set_data(t, v) #移动坐标轴 if(t[-1]&gt;10): plt.xlim(t[-1]-10, t[-1])# save_count为缓存的帧数，# frames 为帧数如果是可迭代对象，可迭代对象的长度将覆盖save_count参数anim = ma.FuncAnimation(plt.gcf(), update, frames=400, interval=40)plt.tight_layout()anim.save('Signal_animation.gif', fps=75,writer='imagemagick')# 如果show 必须放在save后面plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>animation</tag>
        <tag>FuncAnimation</tag>
        <tag>data_generator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.animation.FuncAnimation]]></title>
    <url>%2F2016%2F08%2F12%2Fmatplotlib-animation-FuncAnimation%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445import numpy as npimport matplotlib.pyplot as pltimport matplotlib.animation as ma# 初始化构建所有样本n = 100balls = np.zeros(n, dtype=[ ('position', float, 2), ('size', float, 1), ('growth', float, 1), ('color', float, 4)])balls['position'] = np.random.uniform(0, 1, (n, 2))balls['size'] = np.random.uniform(40, 50, n)balls['growth'] = np.random.uniform(10, 20, n)balls['color'] = np.random.uniform(0, 1, (n, 4))# 绘制图像plt.figure('Bubble', facecolor='lightgray')plt.title('Bubble', fontsize=16)sc = plt.scatter(balls['position'][:, 0], balls['position'][:, 1], balls['size'], color=balls['color'])plt.xticks([])plt.yticks([])# 实现动画def update(number): # 更新界面, 让每个点不断变大 balls['size'] += balls['growth'] sc.set_sizes(balls['size']) index = number % n balls['size'][index] = \ np.random.uniform(40, 70, 1) balls['position'][index] = \ np.random.uniform(0, 1, (1, 2)) # 更新界面 sc.set_sizes(balls['size']) sc.set_offsets(balls['position'])# 每隔三十毫秒，执行一次update函数# 使用imagemagick 保存为gif# 提前 sudo apt-get install imagemagickanim = ma.FuncAnimation( plt.gcf(), update, interval=30, save_count=50)anim.save('bubble_animation.gif',fps=500, writer='imagemagick')plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>animation</tag>
        <tag>FuncAnimation</tag>
        <tag>imagemagick</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python内置函数getitem系列]]></title>
    <url>%2F2016%2F08%2F12%2Fpython%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0getitem%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[123456789101112131415class DictDemo: def __init__(self,key,value): self.dict = &#123;&#125; self.dict[key] = value def __getitem__(self,key): return self.dict[key] def __setitem__(self,key,value): self.dict[key] = value def __len__(self): return len(self.dict)dictDemo = DictDemo('key0','value0')print(dictDemo['key0']) #value0dictDemo['key1'] = 'value1'print(dictDemo['key1']) #value1print(len(dictDemo)) #2]]></content>
      <categories>
        <category>programming/python/内置函数</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>内置函数</tag>
        <tag>getitem</tag>
        <tag>setitem</tag>
        <tag>len</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[from eval to getattr hasattr setattr]]></title>
    <url>%2F2016%2F08%2F12%2Ffrom%20eval%20to%20getattr%20hasattr%20setattr%2F</url>
    <content type="text"><![CDATA[现实中的问题：我手中有一个列表[（’请求网页’,’网页1’）,（’解析网页’,’网页1’, (‘保存网页’, ‘网页1’),’请求网页’,’网页2’）,（’解析网页’,’网页2’, (‘保存网页’, ‘网页2’),… ]，想将列表中的事情从头到尾都干一遍。 首先，定义一个类 1234567class Spider: def request(self, page): print('请求网页:', page) def parse(self, page): print('解析网页:', page) def save(self, page): print('保存网页:', page) 然后我们想的是，如何复用Spider里的方法许多人的做法是直接使用eval做法如下 12345todo_lst = [('request', 'www.exobrain.online'), ('parse', 'www.exobrain.online'), ('save', 'www.exobrain.online'),('request', 'www.exobrain.online/categories/'), ('parse', 'www.exobrain.online/categories/'), ('save', 'www.exobrain.online/categories/'), ('方法测试1','www.exobrain.online/categories/')]spider = Spider()for item in todo_lst: string = 'spider.' + item[0]+'("'+item[1]+'")' eval(string) 出来了结果 123456789请求网页: www.exobrain.online解析网页: www.exobrain.online保存网页: www.exobrain.online请求网页: www.exobrain.online/categories/解析网页: www.exobrain.online/categories/保存网页: www.exobrain.online/categories/...AttributeError: &apos;Spider&apos; object has no attribute &apos;错误的方法测试1&apos;(报错：部分) 我们看到了报错，自然可以用try except处理。but，eval is evil, it’s not suggested. If you use eval, python is no longer elegant.So, we introduce the inbuit methods “getattr, hasattr, setattr“（参考自https://stackoverflow.com/questions/3061/calling-a-function-of-a-module-by-using-its-name-a-string不得不感慨国内外的差距，人家11years前解决的问题，我们到现在还有好多程序员处于问题之中，还在大量使用eval()。） 使用getattr代替eval, 将for部分注释掉重写如下 1234567891011todo_lst = [('request', 'www.exobrain.online'), ('parse', 'www.exobrain.online'), ('save', 'www.exobrain.online'),('request', 'www.exobrain.online/categories/'), ('parse', 'www.exobrain.online/categories/'), ('save', 'www.exobrain.online/categories/'), ('方法测试1','www.exobrain.online/categories/')]spider = Spider()# for item in todo_lst:# string = 'spider.' + item[0]+'("'+item[1]+'")'# eval(string)for item in todo_lst: if not hasattr(spider, item[0]): print('Spider类里不存在方法:', item[0]) continue getattr(spider, item[0])(item[1]) 我们看到了漂亮的结果，有没有发现上面的hasattr+getattr的代码比eval的代码更加优雅。I choose getattr instead of eval just because of elegance. 😃 1234567请求网页: www.exobrain.online解析网页: www.exobrain.online保存网页: www.exobrain.online请求网页: www.exobrain.online/categories/解析网页: www.exobrain.online/categories/保存网页: www.exobrain.online/categories/Spider类里不存在方法: 方法测试1 动态将函数添加到类或者对象里面 12345678910111213141516171819202122def method_added(page): print('调用method_added函数',page)# 将method_added 注册到spider对象里setattr(spider, 'method_added', method_added)# 测试method_added是否添加到spider对象里print(hasattr(spider, 'method_added'))# &gt;True 返回值为True, 添加成功getattr(spider, 'method_added')('www.exobrain.online')# &gt; 调用method_added函数 www.exobrain.onlinedef method_added2(self, page): print('调用method_added2函数',page)# 将method_added2 注册到Spider类里setattr(Spider, 'method_added2', method_added2)# 测试method_added是否添加到spider对象里print(hasattr(spider, 'method_added2'))# &gt;True 返回值为True, 添加成功getattr(spider, 'method_added2')('www.exobrain.online')# &gt; 调用method_added函数 www.exobrain.online 对于函数调用怎么办？我们可以使用locals 和 globals。locals() 和 globals() 是python的两个内置函数，通过它们可以一字典的方式访问局部和全局变量。 1234567891011121314151617def foo(): print "foo"def bar(): print "bar"func_list = ["foo","bar"]for func in func_list: locals()[func]()# &gt;foo# &gt;barfor func in func_list: globals()[func]()# &gt;foo# &gt;bar 参考：https://stackoverflow.com/questions/3061/calling-a-function-of-a-module-by-using-its-name-a-string]]></content>
      <categories>
        <category>programming/python/内置函数</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>eval</tag>
        <tag>getattr</tag>
        <tag>hasattr</tag>
        <tag>setattr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[foo and bar 是个啥？]]></title>
    <url>%2F2016%2F08%2F12%2Ffoo-and-bar-%E6%98%AF%E4%B8%AA%E5%95%A5%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[foo &lt;– fu &lt;– fuck up 一团糟bar &lt;– beyond all recognition![foo_and_bar.jpg]foo_and_bar.jpg]]></content>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.gca:polar]]></title>
    <url>%2F2016%2F08%2F11%2Fmatplotlib-pyplot-gca-polar%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021""" demo09_3d_polar.py 3d 极坐标系"""import matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import axes3dimport numpy as np# 整理数据t = np.linspace(0, 4*np.pi, 1000)r = 0.8 * t# 绘制plt.figure('3D Polar', facecolor='lightgray')plt.title('3D Polar', fontsize=18)ax3d = plt.gca(projection='polar')ax3d.set_xlabel(r'$\theta$', fontsize=14)ax3d.set_ylabel(r'$\rho$', fontsize=14)plt.tick_params(labelsize=10)plt.grid(linestyle=':')plt.plot(t, r)plt.savefig('3d_polar.png')plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pyplot</tag>
        <tag>polar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.plot_wireframe]]></title>
    <url>%2F2016%2F08%2F11%2Fmatplotlib-pyplot-plot-wireframe%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930""" demo07_3d_wireframe.py 3d 曲面图"""import matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import axes3dimport numpy as np# 整理数据n = 500x, y = np.meshgrid(np.linspace(-3, 3, n), np.linspace(-3, 3, n))# print(x, y)# 计算每个坐标点的高度z = (1 - x/2 + x**5 + y**3) * np.exp(-x**2 - y**2)# 绘制plt.figure('3D surface', facecolor='lightgray')plt.title('3D Surface', fontsize=18)ax3d = plt.gca(projection='3d') # class axes3dax3d.set_xlabel('x', fontsize=14)ax3d.set_ylabel('y', fontsize=14)ax3d.set_zlabel('z', fontsize=14)ax3d.plot_wireframe(x, y, z, cmap='jet', rstride=30, cstride=30)plt.tick_params(labelsize=10)# set spines invisible for error of tight_layout()for spine in ax3d.spines.values(): spine.set_visible(False)plt.tight_layout()plt.savefig('3d_wireframe.png')plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pyplot</tag>
        <tag>plot_wireframe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.plot_surface]]></title>
    <url>%2F2016%2F08%2F11%2Fmatplotlib-pyplot-plot-surface%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233""" demo07_3dsurface.py 3d 曲面图"""import matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import axes3dimport numpy as np# 整理数据n = 500x, y = np.meshgrid(np.linspace(-3, 3, n), np.linspace(-3, 3, n))# print(x, y)# 计算每个坐标点的高度z = (1 - x/2 + x**5 + y**3) * np.exp(-x**2 - y**2)# 绘制plt.figure('3D surface', facecolor='lightgray')plt.title('3D Surface', fontsize=18)ax3d = plt.gca(projection='3d')ax3d.set_xlabel('x', fontsize=14)ax3d.set_ylabel('y', fontsize=14)ax3d.set_zlabel('z', fontsize=14)ax3d.plot_surface(x, y, z, cmap='jet', rstride=30, # 行跨距 cstride=30) # 列跨距plt.tick_params(labelsize=10)# set unused spines invisible to fix error of tight_layoutfor spine in ax3d.spines.values(): spine.set_visible(False)plt.tight_layout()plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pyplot</tag>
        <tag>plot_surface</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.3dscatter]]></title>
    <url>%2F2016%2F08%2F11%2Fmatplotlib-pyplot-3dscatter%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930""" demo06_3dscatter.py 3D图"""import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import axes3dn = 300x = np.random.normal(0, 1, n)y = np.random.normal(0, 1, n)z = np.random.normal(0, 1, n)# 绘制三维散点图plt.figure('3D Scatter', facecolor='lightgray')ax3d = plt.gca(projection='3d') # 创建三维坐标系plt.title('3D Scatter', fontsize=20)ax3d.set_xlabel('x', fontsize=14)ax3d.set_ylabel('y', fontsize=14)ax3d.set_zlabel('z', fontsize=14)plt.xticks([-3, -2, -1, 0, 1, 2, 3])plt.yticks([-3, -2, -1, 0, 1, 2, 3])plt.tick_params(labelsize=9)d = np.sqrt(x**2 + y**2 + z**2)ax3d.scatter(x, y, z, marker='o', s=70, c=d, alpha=.5, cmap='jet')# make unused spines invisible to fix error of tight_layoutfor spine in ax3d.spines.values(): spine.set_visible(False)plt.tight_layout()plt.savefig('3d-scatter.png')plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pyplot</tag>
        <tag>3dscatter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交易频率]]></title>
    <url>%2F2016%2F08%2F09%2F%E4%BA%A4%E6%98%93%E9%A2%91%E7%8E%87%2F</url>
    <content type="text"><![CDATA[首先，在交易之前你必须明确每个模型位于交易频谱的哪个频段。以下图为例，股指期货500毫秒级（指行情切片，并非交易频率）的跨期套利属于次高频，这是目前中国市场的天花板；题主所谓的中低频统计套利，应该分布在准低频、次低频以及次长仓的部分。频谱不同位置的模型相关性很低，在组合管理时甚至有对冲之效。所以不存在因为高频套利导致中低频机会减少的状况。link 其次，我们一般从三个维度去考量一个交易模型：盈利，风险，容量。 盈利比较常见的指标是 Annualized Return，风险比较常见的指标是 Maximum Drawdown，也有复合型的比如 Sharpe Ratio，Calmar Ratio，等等。容量则是指该模型所能管理的最大资金规模。在传统的“弱有效市场”假设下，这三个维度的要求无法在单一策略中全部满足。所以我们通常会牺牲其一： 你想要盈利高、容量大，则必须承受不小风险，比如动量类，被认为助涨助跌的趋势策略等；你想要风险低、容量大，则收益率很难上得去，比如机构最流行的期现套利、阿尔法套利等；你想要盈利高、风险低，则不可能容纳大资金，比如题主所说的高频跨期套利。举个例子，股指期货早期的市场微观结构非常粗糙，某个高频模型可以管理2000万人民币左右，平均每天的毛利率约为1%，纯利润大概在2~3‰（由于交易次数多，几乎无单日亏损）；也就是说，每天可以稳定地套到20万左右的毛利，其中约15万作为手续费上缴交易所，自己留下5万养矿工，养码农，必要时还要养监管层的朋友。但随着争夺套利机会的交易者日渐增多，该模型不仅寿命缩短（平均一个月就需要大修），而且管理规模也锐减到数百万的量级，基本相当于残废的水平。如果我想继续用它来管理2000万的账户，则必须付出收益率大幅下降的代价。这就是之前所说的此消彼长。 按照上述归类标准，中低频统计套利属于盈利高、风险大、容量大的策略类型，与高频套利不在同一个次元。假设单笔胜率同为70%，高频每天交易上千笔，根据大数定律，单日胜率几乎收敛到1；而中低频几天做一笔，遇到价差的结构性变化时难免资金回撤，但赚到一笔的利润胜过高频千笔。 正是对这些利润的追求，保证了股指期货各合约之间的价差不会出现大幅的非理性偏离：高频消灭微观上的偏离，而中低频消灭宏观上的偏离。他们各司其职，互相尊重，互不干涉内政。不过话说回来，现在最流行的政治正确，就是高频搞乱了市场，制造了恐慌，必须杀无赦，斩立决。所以题主你如果非要说高频套利太多，导致中低频的机会减少，似乎也并没有什么不对。以上 作者：王不二链接：https://www.zhihu.com/question/33792270/answer/58689716来源：知乎]]></content>
      <categories>
        <category>finance/hedging/concept</category>
      </categories>
      <tags>
        <tag>传统套利</tag>
        <tag>统计套利</tag>
        <tag>趋势套利</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[传统套利、统计套利、趋势套利]]></title>
    <url>%2F2016%2F08%2F09%2F%E4%BC%A0%E7%BB%9F%E5%A5%97%E5%88%A9%E3%80%81%E7%BB%9F%E8%AE%A1%E5%A5%97%E5%88%A9%E3%80%81%E8%B6%8B%E5%8A%BF%E5%A5%97%E5%88%A9%2F</url>
    <content type="text"><![CDATA[传统套利(无风险套利) 传统套利包括价差交易和对冲交易两种类型。统计套利 统计套利是只针对有稳定性的价格关系进行的。 那些没有稳定性的价格关系的套利风险是很大的。价格关系是否稳定直接决定着统计套利能否成立，因此在对价格关系的历史数据进行统计分析的时候，首先要检验价格关系在历史数据中是否稳定。一组价格关系如果是稳定的，那么必定是存在着某一种均衡关系维持机制，一旦价格关系偏离均衡水平，维持机制就会起作用，将价格关系或快或慢地拉回到均衡水平。趋势套利 统计套利和趋势追踪正好是相反的交易理念，趋势追踪讲究只追踪不预测，而统计套利则是从历史数据分析出数据规律，认为该规律会继续延续，对未来进行预测，并通过计算出数据分布的概率来确定交易时机有协整关系的两个标的，价差价比不但有回归特性也可能有趋势特性，也自然的因为品种特性的不同，有的品种趋势特性偏强，而有的品种回归特性偏强 http://blog.sina.com.cn/s/blog_4e08635b0102xv18.html]]></content>
      <categories>
        <category>finance/hedging/concept</category>
      </categories>
      <tags>
        <tag>传统套利</tag>
        <tag>统计套利</tag>
        <tag>趋势套利</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.pie]]></title>
    <url>%2F2016%2F08%2F09%2Fmatplotlib-pyplot-pie%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122""" demo03_pie.py 饼状图"""import numpy as npimport matplotlib.pyplot as plt# 整理数据labels = ['Python', 'JavaScript', 'C++', 'Java', 'PHP']values = [26, 17, 21, 29, 11]spaces = [0.05, 0.01, 0.01, 0.01, 0.01]colors = ['dodgerblue', 'orangered', 'limegreen', 'violet', 'gold']plt.figure('pie', facecolor='lightgray')# title 不显示中文plt.title(r'Pie Figure')# 等轴比例plt.axis('equal')plt.pie(values, spaces, labels, colors, '%.2f%%', shadow=True, startangle=0, radius=1)plt.savefig('Pie.png')plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pyplot</tag>
        <tag>pie</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.contour]]></title>
    <url>%2F2016%2F08%2F09%2Fmatplotlib-pyplot-contour%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526""" demo04_contour.py 绘制等高线图"""import numpy as npimport matplotlib.pyplot as plt# 整理数据n = 500x, y = np.meshgrid(np.linspace(-3, 3, n), np.linspace(-3, 3, n))# print(x, y)# 计算每个坐标点的高度z = (1 - x/2 + x**5 + y**3) * np.exp(-x**2 - y**2)# 绘制等高线plt.figure('Contour', facecolor='lightgray')plt.title('Contour', fontsize=18)plt.grid(linestyle=':')cntr = plt.contour(x, y, z, 8, colors='black', linewidths=.5)# 绘制等高线的高度标签文本# inline_spacing 空白间距plt.clabel(cntr, inline_spacing=1, fmt='%.2f', fontsize=10)# 填充等高线plt.contourf(x, y, z, 8, cmap='jet')plt.savefig('Contour.png')plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pyplot</tag>
        <tag>contour</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.imshow]]></title>
    <url>%2F2016%2F08%2F09%2Fmatplotlib-pyplot-imshow%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021""" demo04_imshow.py 绘制热成像图"""import numpy as npimport matplotlib.pyplot as plt# 整理数据n = 500x, y = np.meshgrid(np.linspace(-3, 3, n), np.linspace(-3, 3, n))# print(x, y)# 计算每个坐标点的高度z = (1 - x/2 + x**5 + y**3) * np.exp(-x**2 - y**2)# 绘制等高线plt.figure('Imshow', facecolor='lightgray')plt.title('Imshow', fontsize=18)plt.grid(linestyle=':')plt.imshow(z, cmap='jet', origin='lower')plt.colorbar()plt.savefig('Imshow.png')plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pyplot</tag>
        <tag>imshow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.plot]]></title>
    <url>%2F2016%2F08%2F09%2Fmatplotlib-pyplot-plot%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.scatter]]></title>
    <url>%2F2016%2F08%2F09%2Fmatplotlib-pyplot-scatter%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.bar]]></title>
    <url>%2F2016%2F08%2F09%2Fmatplotlib-pyplot-bar%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021import numpy as npimport matplotlib.pyplot as pltapples = np.array([91, 86, 23, 89, 45, 62, 39, 84, 88, 99, 21, 33])oranges = np.array([94, 59, 23, 21, 36, 91, 26, 23, 12, 199, 33, 44])plt.figure('Bar Chart', facecolor='lightgray')plt.title('Bar Chart', fontsize=18)plt.xlabel('Month', fontsize=16)plt.ylabel('Volume', fontsize=16)plt.tick_params(labelsize=10)plt.grid(linestyle=':')x = np.arange(1, 13)plt.bar(x-0.2, apples, .4,color='dodgerblue', label='Apple')plt.bar(x+0.2, oranges, .4,color='orangered', label='Orange')# 修改x的刻度文本plt.xticks(x, ['Jan', 'Feb', 'Mar', 'Apr', 'Mar', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])plt.legend()plt.savefig('Bar.png')plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pyplot</tag>
        <tag>bar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot.fill_between]]></title>
    <url>%2F2016%2F08%2F09%2Fmatplotlib-pyplot-fill-between%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324""" demo_01_fill.py plot里的填充"""import numpy as npimport matplotlib.pyplot as pltx = np.linspace(0, 8*np.pi, 1000)sinx = np.sin(x)cosx = np.cos(x/2)/2plt.figure('Fill', facecolor='lightgray')plt.title('Fill', fontsize=18)plt.grid(linestyle=':')plt.plot(x, sinx, color='dodgerblue', label='sin(x)')plt.plot(x, cosx, color='orangered', label='cos(x)')plt.fill_between(x, sinx, cosx, sinx&lt;cosx, color='dodgerblue', alpha=.3)plt.fill_between(x, sinx, cosx, sinx&gt;cosx, color='orangered', alpha=.3)plt.legend()plt.savefig('fill_between.png')plt.show()]]></content>
      <categories>
        <category>programming/python/matplotlib</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pyplot</tag>
        <tag>fill_between</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[real analysis: outer measure]]></title>
    <url>%2F2016%2F08%2F09%2Freal-analysis-outer-measure%2F</url>
    <content type="text"><![CDATA[两种方式从outer measure 到 lebesgue measure1: outer measure + inter measure2: outer measure]]></content>
  </entry>
  <entry>
    <title><![CDATA[real analysis: measure]]></title>
    <url>%2F2016%2F08%2F09%2Freal-analysis-measure%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Foundations of Modern Analysis(Friedman 1970)]]></title>
    <url>%2F2016%2F08%2F08%2FFoundations-of-Modern-Analysis-Friedman-1970%2F</url>
    <content type="text"><![CDATA[台交大吴培元实分析泛函分析课程讲义下载download]]></content>
      <categories>
        <category>math/modern analysis</category>
      </categories>
      <tags>
        <tag>modern analysis</tag>
        <tag>real analysis</tag>
        <tag>functional analysis</tag>
        <tag>textbook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[real analysis and functional analysis 讲义]]></title>
    <url>%2F2016%2F08%2F07%2Freal-analysis-and-functional-analysis%2F</url>
    <content type="text"><![CDATA[台交大吴培元real-analysis-class1.pdfreal-analysis-class10.pdfreal-analysis-class11.pdfreal-analysis-class12.pdfreal-analysis-class13.pdfreal-analysis-class14.pdfreal-analysis-class15.pdfreal-analysis-class16.pdfreal-analysis-class17.pdfreal-analysis-class18.pdfreal-analysis-class19-20.pdfreal-analysis-class2.pdfreal-analysis-class21.pdfreal-analysis-class22.pdfreal-analysis-class23-24.pdfreal-analysis-class25.pdfreal-analysis-class26.pdfreal-analysis-class27.pdfreal-analysis-class28.pdfreal-analysis-class29.pdfreal-analysis-class3.pdfreal-analysis-class30.pdfreal-analysis-class31.pdfreal-analysis-class32.pdfreal-analysis-class33.pdfreal-analysis-class34.pdfreal-analysis-class35.pdfreal-analysis-class36.pdfreal-analysis-class37.pdfreal-analysis-class38.pdfreal-analysis-class39.pdfreal-analysis-class4.pdfreal-analysis-class40.pdfreal-analysis-class41.pdfreal-analysis-class42.pdfreal-analysis-class43.pdfreal-analysis-class44.pdfreal-analysis-class45-46.pdfreal-analysis-class47.pdfreal-analysis-class48.pdfreal-analysis-class49.pdfreal-analysis-class5.pdfreal-analysis-class50.pdfreal-analysis-class52.pdfreal-analysis-class53.pdfreal-analysis-class54-56.pdfreal-analysis-class57.pdfreal-analysis-class58.pdfreal-analysis-class59.pdfreal-analysis-class6.pdfreal-analysis-class60.pdfreal-analysis-class61.pdfreal-analysis-class62.pdfreal-analysis-class63.pdfreal-analysis-class64.pdfreal-analysis-class65.pdfreal-analysis-class66.pdfreal-analysis-class67.pdfreal-analysis-class68.pdfreal-analysis-class69.pdfreal-analysis-class7.pdfreal-analysis-class8.pdfreal-analysis-class9.pdf課程內容.txt]]></content>
      <categories>
        <category>math/real and functional analysis</category>
      </categories>
      <tags>
        <tag>real analysis</tag>
        <tag>functional analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim插件管理器vundle]]></title>
    <url>%2F2016%2F08%2F07%2Fvim%E6%8F%92%E4%BB%B6%E7%AE%A1%E7%90%86%E5%99%A8vundle%2F</url>
    <content type="text"><![CDATA[安装 Vundle 由于 vim 缺乏默认的插件管理器，所有插件的文件都散布在 ~/.vim 下的几个文件夹中，这样导致各种插件的安装、更新、删除都需要自己手动处理，既麻烦费事，又可能出现错误。所以我们需要插件管理器的帮忙，常见的插件管理器有 vundle、pathogen 等等，我们这里使用 vundle。 Vundle 托管在 Github 上，所以使用 git 下载 vundle ，并将其存放于 ~/.vim/bundle/vundle 即可。使用如下命令直接将源代码检出到该目录： 1$ git clone https://github.com/gmarik/vundle.git ~/.vim/bundle/vundle 下载完了 vundle 后，还需要配置 .vimrc 文件。 为了不让 .vimrc 看起来太臃肿，我是新建了一个 ~/.vimrc.bundles 文件来保存所有插件的配置。先在 ~/.vimrc.bundles 文件中包含如下内容： 123filetype off set rtp+=~/.vim/bundle/vundle/call vundle#rc() 然后在 ~/.vimrc 文件中加入内容： 12345678910111213141516171819202122232425262728293031323334353637383940if filereadable(expand("~/.vimrc.bundles")) source ~/.vimrc.bundlesendifset runtimepath^=~/.vim/bundle/ctrlp.vim filetype offset rtp+=~/.vim/bundle/vundle/call vundle#rc()filetype plugin indent on map &lt;C-n&gt; :NERDTreeToggle&lt;CR&gt;set nu highlight LineNr cterm=bold ctermfg=redhighlight StorageClass cterm=bold ctermfg=darkgreenhighlight Type cterm=bold ctermfg=bluehighlight LineNr cterm=bold ctermbg=blackhighlight phpStructure cterm=bold ctermfg=darkredhighlight phpFunctions cterm=bold ctermfg=256highlight Title ctermfg=blue highlight pythonString cterm=bold ctermfg=grayhighlight pythonFunction cterm=bold highlight pythonInclude cterm=bold ctermfg=lightbluehighlight javaScriptStringS ctermfg=gray highlight String ctermfg=grayhi Search cterm=NONE ctermfg=darkred ctermbg=yellow cterm=reverse set hlsearchset backspace=2set rulerset showmodesyntax on set smartindent set tabstop=4 set shiftwidth=4 set expandtab set softtabstop=4 安装插件以“用户名/repos名”的方式。我们这里将插件的配置信息放在 ~/.vimrc.bundles，如下： 12345678910111213141516171819202122232425262728293031323334353637383940filetype off set rtp+=~/.vim/bundle/vundle/call vundle#rc()&quot; Define bundles via Github repos &quot;Bundle &apos;christoomey/vim-run-interactive&apos;Bundle &apos;Valloric/YouCompleteMe&apos;Bundle &apos;croaky/vim-colors-github&apos;Bundle &apos;danro/rename.vim&apos;Bundle &apos;majutsushi/tagbar&apos;Bundle &apos;kchmck/vim-coffee-script&apos;Bundle &apos;kien/ctrlp.vim&apos;Bundle &apos;pbrisbin/vim-mkdir&apos;Bundle &apos;scrooloose/syntastic&apos;Bundle &apos;slim-template/vim-slim&apos;Bundle &apos;thoughtbot/vim-rspec&apos;Bundle &apos;tpope/vim-bundler&apos;Bundle &apos;tpope/vim-endwise&apos;Bundle &apos;tpope/vim-fugitive&apos;Bundle &apos;tpope/vim-rails&apos;Bundle &apos;tpope/vim-surround&apos;Bundle &apos;vim-ruby/vim-ruby&apos;Bundle &apos;vim-scripts/ctags.vim&apos;Bundle &apos;vim-scripts/matchit.zip&apos;Bundle &apos;vim-scripts/tComment&apos;Bundle &apos;mattn/emmet-vim&apos;Bundle &apos;scrooloose/nerdtree&apos;Bundle &apos;Xuyuanp/nerdtree-git-plugin&apos;Bundle &apos;Lokaltog/vim-powerline&apos;Bundle &apos;godlygeek/tabular&apos;Bundle &apos;msanders/snipmate.vim&apos;Bundle &apos;jelera/vim-javascript-syntax&apos;Bundle &apos;altercation/vim-colors-solarized&apos;Bundle &apos;othree/html5.vim&apos;Bundle &apos;xsbeats/vim-blade&apos;Bundle &apos;Raimondi/delimitMate&apos;Bundle &apos;groenewege/vim-less&apos;&quot; Bundle &apos;evanmiller/nginx-vim-syntax&apos;Bundle &apos;Lokaltog/vim-easymotion&apos;Bundle &apos;tomasr/molokai&apos;Bundle &apos;klen/python-mode&apos; 接着，打开 vim，输入 :BundleInstall 或者直接在终端输入 vim +BundleInstall +qall 安装插件。 YouCompleteMe安装YouCompleteMe的步骤 用vundle中的安装太慢,所以自己手动安装检查是否安装clang、g++、cmake等安装插件管理工具 1git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim 在.vimrc中插件管理处加入: 'Valloric/YouCompleteMe'```12安装ycm cd .vim/bundlegit clone https://github.com/Valloric/YouCompleteMe.gitcd YouCompleteMegit submodule update –init –recursive./install.py –clang-completer 12这里安装子模块会出现 fatal: unable to access ‘https://go.googlesource.com/tools/&#39;: Failed to connect to go.googlesource.com port 443: 连接超时fatal: 无法克隆 ‘https://go.googlesource.com/tools&#39; 到子模组路径 ‘/home/zyj/.vim/bundle/YouCompleteMe/third_party/ycmd/third_party/go/src/golang.org/x/tools’ 123原因 go.googlesource.com 域名国内无法直接访问，这时候根据 后面的路径 /home/zyj/.vim/bundle/YouCompleteMe/third_party/ycmd/third_party/go/src/golang.org/x/tools 来知道该模块是要放这个路径下的到 github 上找到该模块下载到该路径 cd ~/.vim/bundle/YouCompleteMe/third_party/ycmd/third_party/go/src/golang.org/xgit clone https://github.com/golang/tools.git 12回到 YouComplateMe 目录继续安装其他子模块 cd ~/.vim/bundle/YouCompleteMe 继续子模块的安装git submodule update –init –recursive 122. 去一个你喜欢的目录，例如用户主目录 cd ~ 创建一个目录用来存放接下来要编译的代码的目录mkdir ~/.ycm_build 进入目录cd ~/.ycm_build 编译输出到当前目录(.ycm_build)1cmake -G &quot;Unix Makefiles&quot; . ~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp 2.1 如果需要实用 C 语言家族的相关功能(我这里系统自带 llvm 3.8、clang 3.8、libclang 没有、libboost-all-dev 没有)，不需要则直接跳过 2.1 1sudo apt install llvm-3.9 clang-3.9 libclang-3.9-dev libboost-all-dev 2.2 编译 YouComplateMe 1# 跳过了 2.1 执行&lt;br&gt;cmake -G &quot;Unix Makefiles&quot; . ~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp&lt;br&gt;&lt;b 在.vimrc中添加下列 1let g:ycm_global_ycm_extra_conf = &apos;~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/.ycm_extra_conf.py&apos;]]></content>
      <categories>
        <category>programming/linux</category>
      </categories>
      <tags>
        <tag>linux vim vundle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[real analysis 01 overrview]]></title>
    <url>%2F2016%2F08%2F06%2Freal-analysis-01-overrview%2F</url>
    <content type="text"><![CDATA[网址:http://v.youku.com/v_show/id_XMjM0Mjg3NDA4.html?f=5422309老师:台交大吴培元吴老师说：这个课听一遍就可以了，知道后续的理论是有基础的，后来的分析遇到了就翻阅相关内容。数学系所说的实变函数学十遍，听听就好。 分析的不同学习阶段，real analysie 位于第二阶段1 数学分析(微积分)2 实分析(实变函数、实变函数论)3 泛函分析 content:1/3 measure theory1/3 lebsgue integral1/3 probility theory Testbook Real Analysis and Functional analysis history measure Riemann integral Lebesgue integral Advantages of Lebesgue over Riemann1) more applicability2) unified theory3)4) fundamental them of Calculus probility theory]]></content>
      <categories>
        <category>math/real-analysis</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>real-analysis</tag>
      </tags>
  </entry>
</search>
